{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stergios-Konstantinidis/DMML2022_Nestle/blob/main/DMML2022_Nestle__.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZKqCFFbNZ04"
      },
      "source": [
        "# Data Mining and Machine Learning - Project\n",
        "\n",
        "## Detecting Difficulty Level of French Texts\n",
        "\n",
        "### Step by step guidelines\n",
        "\n",
        "The following are a set of step by step guidelines to help you get started with your project for the Data Mining and Machine Learning class. \n",
        "To test what you learned in the class, we will hold a competition. You will create a classifier that predicts how the level of some text in French (A1,..., C2). The team with the highest rank will get some goodies in the last class (some souvenirs from tech companies: Amazon, LinkedIn, etc).\n",
        "\n",
        "**2 people per team**\n",
        "\n",
        "Choose a team here:\n",
        "https://moodle.unil.ch/mod/choicegroup/view.php?id=1305831\n",
        "\n",
        "\n",
        "#### 1. ðŸ“‚ Create a public GitHub repository for your team using this naming convention `DMML2022_[your_team_name]` with the following structure:\n",
        "- data (folder) \n",
        "- code (folder) \n",
        "- documentation (folder)\n",
        "- a readme file (.md): *mention team name, participants, brief description of the project, approach, summary of results table and link to the explainatory video (see below).*\n",
        "\n",
        "All team members should contribute to the GitHub repository.\n",
        "\n",
        "#### 2. ðŸ‡° Join the competititon on Kaggle using the invitation link we sent on Slack.\n",
        "\n",
        "Under the Team tab, save your team name (`UNIL_your_team_name`) and make sure your team members join in as well. You can merge your user account with your teammates in order to create a team.\n",
        "\n",
        "#### 3. ðŸ““ Read the data into your colab notebook. There should be one code notebook per team, but all team members can participate and contribute code. \n",
        "\n",
        "You can use either direct the Kaggle API and your Kaggle credentials (as explained below and **entirely optional**), or dowload the data form Kaggle and upload it onto your team's GitHub repository under the data subfolder.\n",
        "\n",
        "#### 4. ðŸ’Ž Train your models and upload the code under your team's GitHub repo. Set the `random_state=0`.\n",
        "- baseline\n",
        "- logistic regression with TFidf vectoriser (simple, no data cleaning)\n",
        "- KNN & hyperparameter optimisation (simple, no data cleaning)\n",
        "- Decision Tree classifier & hyperparameter optimisation (simple, no data cleaning)\n",
        "- Random Forests classifier (simple, no data cleaning)\n",
        "- another technique or combination of techniques of your choice\n",
        "\n",
        "BE CREATIVE! You can use whatever method you want, in order to climb the leaderboard. The only rule is that it must be your own work. Given that, you can use all the online resources you want. \n",
        "\n",
        "#### 5. ðŸŽ¥ Create a YouTube video (10-15 minutes) of your solution and embed it in your notebook. Explain the algorithms used and the evaluation of your solutions. *Select* projects will also be presented live by the group during the last class.\n",
        "\n",
        "\n",
        "### Submission details (one per team)\n",
        "\n",
        "1. Download a ZIPped file of your team's repository and submit it in Moodle here. IMPORTANT: in the comment of the submission, insert a link to the repository on Github.\n",
        "https://moodle.unil.ch/mod/assign/view.php?id=1305833\n",
        "\n",
        "\n",
        "\n",
        "### Grading (one per team)\n",
        "- 20% Kaggle Rank\n",
        "- 50% code quality (using classes, splitting into proper files, documentation, etc)\n",
        "- 15% github quality (include link to video, table with progress over time, organization of code, images, etc)\n",
        "- 15% video quality (good sound, good slides, interesting presentation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-14CAdOoinM"
      },
      "source": [
        "## Some further details for points 3 and 4 above.\n",
        "\n",
        "### 3. Read data into your notebook with the Kaggle API (optional but useful). \n",
        "\n",
        "You can also download the data from Kaggle and put it in your team's repo the data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ_hnJzSNO2g",
        "outputId": "1178fe14-72a1-4d64-8a59-5d92eb462e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# reading in the data via the Kaggle API\n",
        "\n",
        "# mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJPTz3D7TeQv",
        "outputId": "49c91521-143e-4600-9ad7-59e18ed9441d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# install Kaggle\n",
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKG1TCddRYTB"
      },
      "source": [
        "### IMPORTANT\n",
        "Log into your Kaggle account, go to Account > API > Create new API token. You will obtain a kaggle.json file. Save it in your Google Drive (not in a folder, in your general drive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JgzLj451YDfV"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KrsZLalrSI3u"
      },
      "outputs": [],
      "source": [
        "#read in your Kaggle credentials from Google Drive\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-ETUrrhgdnfU"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BDI60LXKTPzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fddb09b-f545-4d08-fce6-cb394ee2d5c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading detecting-french-texts-difficulty-level-2022.zip to /content\n",
            "\r  0% 0.00/303k [00:00<?, ?B/s]\n",
            "\r100% 303k/303k [00:00<00:00, 107MB/s]\n",
            "Archive:  detecting-french-texts-difficulty-level-2022.zip\n",
            "  inflating: data/sample_submission.csv  \n",
            "  inflating: data/training_data.csv  \n",
            "  inflating: data/unlabelled_test_data.csv  \n"
          ]
        }
      ],
      "source": [
        "# download the dataset from the competition page\n",
        "\n",
        "try:\n",
        "  df = pd.read_csv('/content/data/training_data.csv')\n",
        "except:\n",
        "  !kaggle competitions download -c detecting-french-texts-difficulty-level-2022\n",
        "  !unzip \"detecting-french-texts-difficulty-level-2022.zip\" -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "daqvj7feTx60"
      },
      "outputs": [],
      "source": [
        "# read in your training data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn \n",
        "#import sklearn.model_selection\n",
        "\n",
        "df = pd.read_csv('/content/data/training_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VxRSnk5bhTp8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d709403a-8d1c-4e4d-ee43-b2ef1e6f5594"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           sentence difficulty\n",
              "0   0  Les coÃ»ts kilomÃ©triques rÃ©els peuvent diverger...         C1\n",
              "1   1  Le bleu, c'est ma couleur prÃ©fÃ©rÃ©e mais je n'a...         A1\n",
              "2   2  Le test de niveau en franÃ§ais est sur le site ...         A1\n",
              "3   3           Est-ce que ton mari est aussi de Boston?         A1\n",
              "4   4  Dans les Ã©coles de commerce, dans les couloirs...         B1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d5909c8-5425-4528-8954-eaa05bc32cf0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Les coÃ»ts kilomÃ©triques rÃ©els peuvent diverger...</td>\n",
              "      <td>C1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Le bleu, c'est ma couleur prÃ©fÃ©rÃ©e mais je n'a...</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Le test de niveau en franÃ§ais est sur le site ...</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Dans les Ã©coles de commerce, dans les couloirs...</td>\n",
              "      <td>B1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d5909c8-5425-4528-8954-eaa05bc32cf0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d5909c8-5425-4528-8954-eaa05bc32cf0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d5909c8-5425-4528-8954-eaa05bc32cf0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kpfbtndj0jL"
      },
      "source": [
        "Have a look at the data on which to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7G75Q1gRj49l",
        "outputId": "500fc6ed-e21a-4f50-b184-35ec8d7341a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           sentence\n",
              "0   0  Nous dÃ»mes nous excuser des propos que nous eÃ»...\n",
              "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
              "2   2  Et, paradoxalement, boire froid n'est pas la b...\n",
              "3   3  Ce n'est pas Ã©tonnant, car c'est une saison my...\n",
              "4   4  Le corps de Golo lui-mÃªme, d'une essence aussi..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-173ee8e4-67df-4629-9795-1e7e2b7e78ab\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Nous dÃ»mes nous excuser des propos que nous eÃ»...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Et, paradoxalement, boire froid n'est pas la b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Ce n'est pas Ã©tonnant, car c'est une saison my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Le corps de Golo lui-mÃªme, d'une essence aussi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-173ee8e4-67df-4629-9795-1e7e2b7e78ab')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-173ee8e4-67df-4629-9795-1e7e2b7e78ab button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-173ee8e4-67df-4629-9795-1e7e2b7e78ab');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_pred = pd.read_csv('/content/data/unlabelled_test_data.csv')\n",
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a37hWJ_ckBlk"
      },
      "source": [
        "And this is the format for your submissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gk9H2dLHkFBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "14f631c6-92c1-4271-8d63-024c09fcda20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id difficulty\n",
              "0   0         A1\n",
              "1   1         A1\n",
              "2   2         A1\n",
              "3   3         A1\n",
              "4   4         A1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-135162f3-bc6a-435b-b012-03838c7e8a83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-135162f3-bc6a-435b-b012-03838c7e8a83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-135162f3-bc6a-435b-b012-03838c7e8a83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-135162f3-bc6a-435b-b012-03838c7e8a83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df_example_submission = pd.read_csv('/content/data/sample_submission.csv')\n",
        "df_example_submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfTgL1erjqQ6"
      },
      "source": [
        "### 4. Train your models\n",
        "\n",
        "Set your X and y variables. \n",
        "Set the `random_state=0`\n",
        "Split the data into a train and test set using the following parameters `train_test_split(X, y, test_size=0.2, random_state=0)`.\n",
        "\n",
        "#### 4.1.Baseline\n",
        "What is the baseline for this classification problem?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install -U scikit-learn\n",
        "#pip install numpy \n",
        "#pip install scipy"
      ],
      "metadata": {
        "id": "1sqqpuY8PZbR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVEaPzxuQFxg",
        "outputId": "21e65eed-bd14-45be-9163-441417ac9a2c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvw9eWR4QLM6",
        "outputId": "bbfb2962-de2f-449f-fc36-37a2ef7af3e8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "import spacy.cli\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
        "import numpy as np\n",
        "from sklearn import linear_model, decomposition, datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "EXJ03SkaWj4N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "t4O_pYiHpiRd"
      },
      "outputs": [],
      "source": [
        "np.random.seed = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WDdFr4xsk5Qf"
      },
      "outputs": [],
      "source": [
        "#Split data set\n",
        "\n",
        "X= df['sentence']\n",
        "y= df['difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlvbPYa0k78l"
      },
      "source": [
        "#### 4.2. Logistic Regression (without data cleaning)\n",
        "\n",
        "Train a simple logistic regression model using a Tfidf vectoriser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEe3-QNlow4H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "a1d6b49b-e837-4ac4-f378-c1ef7f50e21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      000  02h00  03h00   10  100  1000  10000  105   11  110  ...  Ã©vÃ©nement  \\\n",
              "0     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "1     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "2     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "3     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "...   ...    ...    ...  ...  ...   ...    ...  ...  ...  ...  ...        ...   \n",
              "4795  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4796  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4797  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4798  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4799  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "\n",
              "      Ã©vÃ©nements  Ãªtes  Ãªtre  Ãªtres  Ãªut  Ã®le  Ã®les  Ã´ta  Ã´ter  \n",
              "0       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "1       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "2       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "3       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "...          ...   ...   ...    ...  ...  ...   ...  ...   ...  \n",
              "4795    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4796    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4797    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4798    0.200821   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4799    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "\n",
              "[4800 rows x 14585 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce3ad961-1b17-45b2-ace9-47b84f9932dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>02h00</th>\n",
              "      <th>03h00</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>10000</th>\n",
              "      <th>105</th>\n",
              "      <th>11</th>\n",
              "      <th>110</th>\n",
              "      <th>...</th>\n",
              "      <th>Ã©vÃ©nement</th>\n",
              "      <th>Ã©vÃ©nements</th>\n",
              "      <th>Ãªtes</th>\n",
              "      <th>Ãªtre</th>\n",
              "      <th>Ãªtres</th>\n",
              "      <th>Ãªut</th>\n",
              "      <th>Ã®le</th>\n",
              "      <th>Ã®les</th>\n",
              "      <th>Ã´ta</th>\n",
              "      <th>Ã´ter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4798</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.200821</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4799</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4800 rows Ã— 14585 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce3ad961-1b17-45b2-ace9-47b84f9932dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce3ad961-1b17-45b2-ace9-47b84f9932dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce3ad961-1b17-45b2-ace9-47b84f9932dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "i529gMyojD1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-qO8C5oyov"
      },
      "source": [
        "Calculate accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoder ou label encoder \n",
        "\n",
        "df['oe_difficulty'] = ['0' if x == 'A1'\n",
        "                   else '1' if x =='A2'\n",
        "                   else '2' if x == 'B1'\n",
        "                   else '3' if x=='B2'\n",
        "                   else '4' if x== 'C1'\n",
        "                   else '5'\n",
        "                   for x in df.difficulty]\n",
        "\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)\n",
        "\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "muF_24BsZ1ub"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PViIQdnDpASy"
      },
      "outputs": [],
      "source": [
        "LR = LogisticRegressionCV(solver='lbfgs', cv=16, max_iter=1000, random_state = 50)\n",
        "\n",
        "LR.fit(X_train_text, y_train)\n",
        "\n",
        "LR.score(X_train_text, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accur_test_LR = LR.score(X_test_text, y_test)\n",
        "accur_test_LR"
      ],
      "metadata": {
        "id": "c1-dsTH4j4fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = LR.predict(X_test_text)\n",
        "precision_score_LR_test =  \"Precision Score : \",precision_score(y_test, y_pred, \n",
        "                                           pos_label='positive',\n",
        "                                           average='micro')\n",
        "recall_score_LR_test = \"Recall Score : \",recall_score(y_test, y_pred, \n",
        "                                           pos_label='positive',\n",
        "                                           average='micro')\n",
        "print(precision_score_LR_test)\n",
        "print(recall_score_LR_test)"
      ],
      "metadata": {
        "id": "hS-CY6xF_mWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for text in X_test_text:\n",
        "  print((X_test.reset_index())._get_value(i, \"sentence\", takeable=False) + \"  \" + LR.predict(text))\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "1gQPHn8pV0pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred = pd.read_csv('/content/data/unlabelled_test_data.csv')\n",
        "df_pred_text = text_transformer.transform(df_pred[\"sentence\"])\n",
        "\n",
        "df_pred['difficulty'] = list(map(lambda x : LR.predict(x)[0], df_pred_text))\n",
        "\n",
        "df_pred = df_pred[[\"id\", \"difficulty\"]]\n",
        "df_pred = df_pred.set_index('id')\n",
        "\n",
        "df_pred.to_csv('file_name.csv')\n",
        "df_pred.head()\n"
      ],
      "metadata": {
        "id": "MghtmHHlURok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D3_dp3apcmr"
      },
      "source": [
        "Have a look at the confusion matrix and identify a few examples of sentences that are not well classified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TTEiuXasNFg"
      },
      "source": [
        "Generate your first predictions on the `unlabelled_test_data.csv`. make sure your predictions match the format of the `unlabelled_test_data.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXG_yIG_pQ8t"
      },
      "source": [
        "#### 4.3. KNN (without data cleaning)\n",
        "\n",
        "Train a KNN classification model using a Tfidf vectoriser. Show the accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPRjD1rSqKKZ"
      },
      "outputs": [],
      "source": [
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoder ou label encoder \n",
        "\n",
        "df['oe_difficulty'] = ['0' if x == 'A1'\n",
        "                   else '1' if x =='A2'\n",
        "                   else '2' if x == 'B1'\n",
        "                   else '3' if x=='B2'\n",
        "                   else '4' if x== 'C1'\n",
        "                   else '5'\n",
        "                   for x in df.difficulty]\n",
        "\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "4y9EVvgqZJSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6rH2Hx0qtB2"
      },
      "source": [
        "Try to improve it by tuning the hyper parameters (`n_neighbors`,   `p`, `weights`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRy18Ce_qxPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5757b6-117f-4630-8930-2cc81dae67d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 19 candidates, totalling 190 fits\n"
          ]
        }
      ],
      "source": [
        "#KNN with 6 neightbors accuracy 0.2073\n",
        "#KNN with 8 Neightbors accuracy 0.2073\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "#K in range from 1 to 8\n",
        "k_range = list(range(1, 20))\n",
        "param_grid = dict(n_neighbors=k_range)\n",
        "\n",
        "# defining parameter range\n",
        "knn_cv = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1)\n",
        "  \n",
        "# fitting the model for grid search\n",
        "knn_cv.fit(X_train_text, y_train)\n",
        "\n",
        "y_pred_knn = knn_cv.predict(X_test_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN with minkowski and algorithm brute give us accuracy of: 0.1719\n",
        "KNeighborsClassifier(algorithm='brute', leaf_size=10, metric='minkowski',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
        "                     weights='uniform')\n",
        "knn.fit(X_train_text, y_train)\n",
        "\n",
        "y_pred_knn = knn.predict(X_test_text)"
      ],
      "metadata": {
        "id": "0PV9gUSySFY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_test = knn.score(X_test_text, y_test)\n",
        "knn_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eFtTuV2ZU0Y",
        "outputId": "3f5d833d-ad39-474e-b79a-d7a2a2f6c094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3229166666666667"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.32291"
      ],
      "metadata": {
        "id": "y1C6ssd9Zu_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#avec micro partout la meme valeur 0.2073\n",
        "#avec macro Precision , recall et Score different\n",
        "def evaluate(test, pred):\n",
        "  precision = precision_score(test, pred, pos_label='positive',\n",
        "                                           average='macro')\n",
        "  recall = recall_score(test, pred, pos_label='positive',\n",
        "                                           average='macro')\n",
        "  f1= f1_score(test, pred, pos_label='positive',\n",
        "                                           average='macro')\n",
        "  print(f\"ACCURACY SCORE:\\n{accuracy_score(test, pred) :.4f}\")\n",
        "  print(f'CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}')\n",
        "\n",
        "evaluate(y_test, y_pred_knn)"
      ],
      "metadata": {
        "id": "Gb7hfqgVXuXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5112e7-0a8c-47b6-f682-07095ada8b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3229\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3927\n",
            "\tRecall: 0.3232\n",
            "\tF1_Score: 0.3076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  average_options = (None, \"micro\", \"macro\", \"weighted\", \"samples\")\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  average_options = (None, \"micro\", \"macro\", \"weighted\", \"samples\")\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  average_options = (None, \"micro\", \"macro\", \"weighted\", \"samples\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFNH1WgNqc62"
      },
      "source": [
        "#### 4.4. Decision Tree Classifier (without data cleaning)\n",
        "\n",
        "Train a Decison Tree classifier, using a Tfidf vectoriser. Show the accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQHjvOp7q11L"
      },
      "source": [
        "Try to improve it by tuning the hyper parameters (`max_depth`, the depth of the decision tree)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Fzl5BUq8JN"
      },
      "outputs": [],
      "source": [
        "# first we tried depth 10, and accuracy of 0.1885\n",
        "# with deph=16 we have accuracy =0.1969\n",
        "# with deph=26 accuracy = 0.2042\n",
        "# wuth deph =40. accuracy=0.2021\n",
        "\n",
        "tree = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                       max_depth=4000, \n",
        "                       random_state=50)\n",
        "tree.fit(X_train_text, y_train)\n",
        "y_pred_tree = tree.predict(X_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(train, pred):\n",
        "  precision = precision_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  recall = recall_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  f1= f1_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  print(f\"ACCURACY SCORE:\\n{accuracy_score(train, pred) :.4f}\")\n",
        "  print(f'CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}')\n",
        "evaluate(y_test, y_pred_tree)"
      ],
      "metadata": {
        "id": "0hGXj6RaaSAU",
        "outputId": "bac47f96-78d7-4a50-ab6d-ed8a32a3c6e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3021\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3021\n",
            "\tRecall: 0.3021\n",
            "\tF1_Score: 0.3021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M52Ys3hcq7ku"
      },
      "source": [
        "#### 4.5. Random Forest Classifier (without data cleaning)\n",
        "\n",
        "Try a Random Forest Classifier, using a Tfidf vectoriser. Show the accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoder ou label encoder \n",
        "\n",
        "df['oe_difficulty'] = ['0' if x == 'A1'\n",
        "                   else '1' if x =='A2'\n",
        "                   else '2' if x == 'B1'\n",
        "                   else '3' if x=='B2'\n",
        "                   else '4' if x== 'C1'\n",
        "                   else '5'\n",
        "                   for x in df.difficulty]\n",
        "\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "hMgVlrBb-wTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())"
      ],
      "metadata": {
        "id": "gw_8yLbsAvuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "6ptuGy7SBE_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "## Doesnt work because it's for continuous varibale like housing price \n",
        "\n",
        "#rf = RandomForestRegressor(random_state = 42)\n",
        "#from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "#print('Parameters currently in use:\\n')\n",
        "#pprint(rf.get_params())"
      ],
      "metadata": {
        "id": "zbvloIiy7coB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "OcJh0i-y7krX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf408f6-5b4e-4378-b4ab-acfa61d5d089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "\n",
        "# Accuracy less than 50\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_clf = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 4, cv = 2, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_clf.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "id": "8XZqDNxD7oFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_test = rf_clf.score(X_test_text, y_test)\n",
        "rf_clf_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug4FBMTZUlAU",
        "outputId": "aa0b1048-335b-42a3-9967-d5c90c24db52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40625"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Marche beaucoup moins bien\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_clf = RandomForestClassifier(criterion='gini',\n",
        "                                 n_estimators=5,\n",
        "                                 random_state=42,\n",
        "                                 n_jobs=20,\n",
        "                                max_depth=40,\n",
        "                                min_samples_split=5,\n",
        "                                max_features=30000,\n",
        "                                bootstrap = bool,\n",
        "                                oob_score= bool,\n",
        "                                warm_start= bool)\n",
        "# Fit the random search model\n",
        "rf_clf.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkEw2RuzUAi7",
        "outputId": "5e2a23e1-818d-4c76-f2b0-bb0034f15dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py:560: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
            "  # axis to be consistent with the classification case and make\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=<class 'bool'>, max_depth=40,\n",
              "                       max_features=30000, min_samples_split=5, n_estimators=5,\n",
              "                       n_jobs=20, oob_score=<class 'bool'>, random_state=42,\n",
              "                       warm_start=<class 'bool'>)"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_test = rf_clf.score(X_test_text, y_test)\n",
        "rf_clf_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ebo8JJYXFyf",
        "outputId": "6ca54cc5-3f40-48a5-ceb4-57f6afbd078f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.325"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "pTPHTA-8C4jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_test = rf_clf.score(X_test_text, y_test)\n",
        "rf_clf_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1jk8RyOBwsN",
        "outputId": "0d19dfff-fc8b-4c99-a499-f2f476111991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40625"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_features, test_labels):\n",
        "    predictions = model.predict(test_features)\n",
        "    errors = abs(predictions - test_labels)\n",
        "    mape = 100 * np.mean(errors / test_labels)\n",
        "    accuracy = 100 - mape\n",
        "    print('Model Performance')\n",
        "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
        "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
        "base_model.fit(X_train_text, y_train)\n",
        "base_accuracy = evaluate(base_model, X_test_text, y_test)\n",
        "\n",
        "\n",
        "best_random = rf_random.best_estimator_\n",
        "random_accuracy = evaluate(best_random, X_test_text, y_test)\n",
        "\n",
        "\n",
        "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n"
      ],
      "metadata": {
        "id": "1BW7KfuTBsvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [80, 90, 100, 110],\n",
        "    'max_features': [2, 3],\n",
        "    'min_samples_leaf': [3, 4, 5],\n",
        "    'min_samples_split': [8, 10, 12],\n",
        "    'n_estimators': [100, 200, 300, 1000]\n",
        "}\n",
        "# Create a based model\n",
        "rf = RandomForestRegressor()\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "grid_search = GridSearchCV(estimator = RandomForestRegressor(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
        "grid_search.fit(X_train_text, y_train)\n",
        "grid_search.best_params_\n",
        "best_grid = grid_search.best_estimator_\n",
        "grid_accuracy = evaluate(best_grid, X_test_text, y_test)\n",
        "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
      ],
      "metadata": {
        "id": "8ANhf-Eg7MlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sssF4NIGrNLa",
        "outputId": "13308591-ea2e-464a-8a62-43adeada6d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-b5306b7fed00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                           \u001b[0;31m#n_informative=2, n_redundant=0,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                           \u001b[0;31m#random_state=0, shuffle=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
          ]
        }
      ],
      "source": [
        "#X, y = make_classification(n_samples=1000, n_features=4,\n",
        "                          #n_informative=2, n_redundant=0,\n",
        "                          #random_state=0, shuffle=False)\n",
        "\n",
        "\n",
        "clf = RandomForestClassifier(random_state=0, bootstrap= True, max_depth= 80,\n",
        " max_features= 3, min_samples_leaf =5, n_estimators= 100)\n",
        "\n",
        "clf.fit(X_train_text, y_train)\n",
        "RandomForestClassifier(...)\n",
        "\n",
        "y_pred_forest =  clf.predict(X_test_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(train, pred):\n",
        "  precision = precision_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  recall = recall_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  f1= f1_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  print(f'CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}')\n",
        "evaluate(y_test, y_pred_forest)"
      ],
      "metadata": {
        "id": "K1F5ZCGgcjeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7329656e-bb39-4fb5-d1b0-2d75391c001e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.1646\n",
            "\tRecall: 0.1646\n",
            "\tF1_Score: 0.1646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_train_text, y_train)\n",
        "accur_train_forest = clf.score(X_train_text, y_train)\n",
        "accur_train_forest"
      ],
      "metadata": {
        "id": "u8qnuSbjaV_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_test_text, y_pred)\n",
        "accur_test_forest = clf.score(X_test_text, y_test)\n",
        "accur_test_forest"
      ],
      "metadata": {
        "id": "xpjEP1YtbCGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-8_3MK1rpZr"
      },
      "source": [
        "#### 4.6. Any other technique, including data cleaning if necessary\n",
        "\n",
        "Try to improve accuracy by training a better model using the techniques seen in class, or combinations of them.\n",
        "\n",
        "As usual, show the accuracy, precision, recall and f1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first let's check if there are NA's\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gIJjicHh2B_",
        "outputId": "9fefab04-6e80-4ffa-ea54-0fd13acec6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id            0\n",
              "sentence      0\n",
              "difficulty    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le_diff = pd.Series(LabelEncoder().fit_transform(df[\"difficulty\"]), name=\"le_diff\")\n",
        "le_diff"
      ],
      "metadata": {
        "id": "jC8R5PMViQDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7360edbf-541b-4f60-ef4c-736bba19ea43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       4\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       2\n",
              "       ..\n",
              "4795    3\n",
              "4796    4\n",
              "4797    1\n",
              "4798    5\n",
              "4799    5\n",
              "Name: le_diff, Length: 4800, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82FvnJycsBFf"
      },
      "source": [
        "#### 4.7. Show a summary of your results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tableau_data = {'Logistic Regression': [0.565376, accur_test_LR, accur_train_LR,  precision_score_LR_train, recall_score_LR_train],\n",
        "        'KNN': [0.565376, accur_test_KNN , accur_train_KNN, precision_score_KNN, recall_score_KNN],\n",
        "        'Decision Tree' : [accur_test_tree, accur_train_tree, precision_score_tree, recall_score_tree]\n",
        "        'Random Forest': [accur_test_forest, accur_train_forest, precision_score_forest, recall_score_forest]}\n",
        "  \n",
        "# Creates pandas DataFrame.\n",
        "tableau_df = pd.DataFrame(tableau_data, index=[ 'Base rate', 'Accuracy Test',\n",
        "                               'Accuracy Train',\n",
        "                               'Precision',\n",
        "                               'Recall'])\n",
        "\n",
        "tableau_df"
      ],
      "metadata": {
        "id": "GtoMd99OU9lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate the encoder\n",
        "oe=OrdinalEncoder()\n",
        "\n",
        "# set the order of your categories\n",
        "oe.set_params(categories= [['0', '1', '2', '3', '4','5']])\n",
        "\n",
        "# fit-transform a dataframe of the categorical age variable\n",
        "oe_difficulty =oe.fit_transform(df[['oe_difficulty']])\n",
        "\n",
        "#number of values per class\n",
        "oe_difficulty = pd.DataFrame(oe_difficulty).astype('int')\n",
        "oe_difficulty.value_counts()"
      ],
      "metadata": {
        "id": "-8FFD5uDy-fv",
        "outputId": "f650ada9-5fc8-48d1-e023-f3607cb23ce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    813\n",
              "5    807\n",
              "4    798\n",
              "1    795\n",
              "2    795\n",
              "3    792\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### NEW TEST\n",
        "\n",
        "#Encode Data\n",
        "df['oe_difficulty'] = [0 if x == 'A1'\n",
        "                   else 2 if x =='A2'\n",
        "                   else 1 if x== 'B1'\n",
        "                   else 3 if x == 'B2'\n",
        "                   else 4 if x == 'C1'\n",
        "                   else 5\n",
        "                   for x in df.difficulty]\n",
        "df.drop(labels='difficulty', axis=1)\n",
        "\n",
        "#Encode column\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "#Encoded dataframe\n",
        "df_encoded= df.drop('difficulty', axis = 1)\n",
        "df_encoded\n",
        "\n",
        "#Vectorize\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "6742PIoRdWpd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pca = decomposition.PCA()\n",
        "#std_slc = StandardScaler()\n",
        "#logistic_Reg = linear_model.LogisticRegression()\n",
        "#pipe = Pipeline(steps=[('std_slc', std_slc),\n",
        "                           #('pca', pca),\n",
        "                           #('logistic_Reg', logistic_Reg)])"
      ],
      "metadata": {
        "id": "KiEWpL8BZp4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA\n",
        "#from sklearn.decomposition import PCA\n",
        "#pca = PCA(n_components=2)\n",
        "#pca.fit(X_train_text)\n",
        "#PCA(n_components=2)\n",
        "#print(pca.explained_variance_ratio_)\n",
        "#print(pca.singular_values_)\n"
      ],
      "metadata": {
        "id": "NlTVkN0WbWbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##NEW TEST\n",
        "\n",
        "#Vectorize\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())\n",
        "\n",
        "#Transform\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "#Split \n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCd2BNvXY4DG",
        "outputId": "8941b90a-489c-47a6-c6f7-69fa0102a009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR_cv = LogisticRegressionCV(solver='lbfgs', cv=10, max_iter=100, random_state = 0)\n",
        "\n",
        "LR_cv.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "id": "HSwzwt_L0pNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_accur_test = LR_cv.score(X_test_text, y_test)\n",
        "LR_accur_test"
      ],
      "metadata": {
        "id": "E3qCkfbYgGBz",
        "outputId": "de2fde0c-7127-4e57-b605-aa411303a8db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46458333333333335"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solver='lbfgs', \n",
        "#cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.5520833333333334\n",
        "\n",
        "#With vectorizer\n",
        "#solver='lbfgs', \n",
        "#cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.5520833333333334\n",
        "\n",
        "\n",
        "#solver='newton-cholesky', \n",
        "#cv=10\n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "#took to much time\n",
        "\n",
        "\n",
        "#solver='lbfgs'\n",
        "# cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.46458333333333335\n",
        "\n",
        "\n",
        "#Vectorize\n",
        "#Encoded Y\n",
        "#solver='lbfgs'\n",
        "# cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.553125\n",
        "\n",
        "#Vectorize\n",
        "#Encoded Y TRUUUUUE\n",
        "#solver='lbfgs'\n",
        "# cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.46458333333333335"
      ],
      "metadata": {
        "id": "9ovRqV_5VeYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fcbe2f-04a6-4bac-c926-0135880d4bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.553125"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW TEST\n",
        "\n",
        "LR_cv = LogisticRegressionCV(solver='newton-cholesky', cv=10, max_iter=100, random_state = 0)\n",
        "\n",
        "LR_cv.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "id": "NrdMx6FrUEJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_accur_test = LR_cv.score(X_test_text, y_test)\n",
        "LR_accur_test"
      ],
      "metadata": {
        "id": "6PPjWiz7VvBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "##NEW TEST\n",
        "\n",
        "#Vectorize\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())\n",
        "\n",
        "#Transform\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "X_text = text_transformer.transform(X)\n",
        "\n",
        "#Split \n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "ISHYXWcE_0J3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e873a9-ce63-4fb3-cfc4-9a207d497e06"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "\n",
        "###BERT MODEL\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "#Import bert model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Load the tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "oJQN5uoJXU4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d27c48-b0b4-423b-9ef0-1b24196e5228"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "df_one = df.drop(['difficulty'], axis=1)\n",
        "df_one['oe_difficulty'].value_counts(normalize = True)\n",
        "df_one\n"
      ],
      "metadata": {
        "id": "S_FEeyS8keUb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "fd327d56-7a6e-48d7-a599-c96fc2e4c455"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                           sentence  oe_difficulty\n",
              "0        0  Les coÃ»ts kilomÃ©triques rÃ©els peuvent diverger...              4\n",
              "1        1  Le bleu, c'est ma couleur prÃ©fÃ©rÃ©e mais je n'a...              0\n",
              "2        2  Le test de niveau en franÃ§ais est sur le site ...              0\n",
              "3        3           Est-ce que ton mari est aussi de Boston?              0\n",
              "4        4  Dans les Ã©coles de commerce, dans les couloirs...              1\n",
              "...    ...                                                ...            ...\n",
              "4795  4795  C'est pourquoi, il dÃ©cida de remplacer les hab...              3\n",
              "4796  4796  Il avait une de ces pÃ¢leurs splendides qui don...              4\n",
              "4797  4797  Et le premier samedi de chaque mois, venez ren...              2\n",
              "4798  4798  Les coÃ»ts liÃ©s Ã  la journalisation n'Ã©tant pas...              5\n",
              "4799  4799  Sur le sable, la mer haletait de toute la resp...              5\n",
              "\n",
              "[4800 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4e3375ba-da1d-40eb-b644-f6b7c35339c3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "      <th>oe_difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Les coÃ»ts kilomÃ©triques rÃ©els peuvent diverger...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Le bleu, c'est ma couleur prÃ©fÃ©rÃ©e mais je n'a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Le test de niveau en franÃ§ais est sur le site ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Est-ce que ton mari est aussi de Boston?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Dans les Ã©coles de commerce, dans les couloirs...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>4795</td>\n",
              "      <td>C'est pourquoi, il dÃ©cida de remplacer les hab...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>4796</td>\n",
              "      <td>Il avait une de ces pÃ¢leurs splendides qui don...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>4797</td>\n",
              "      <td>Et le premier samedi de chaque mois, venez ren...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4798</th>\n",
              "      <td>4798</td>\n",
              "      <td>Les coÃ»ts liÃ©s Ã  la journalisation n'Ã©tant pas...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4799</th>\n",
              "      <td>4799</td>\n",
              "      <td>Sur le sable, la mer haletait de toute la resp...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4800 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e3375ba-da1d-40eb-b644-f6b7c35339c3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4e3375ba-da1d-40eb-b644-f6b7c35339c3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4e3375ba-da1d-40eb-b644-f6b7c35339c3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "#lenghts of messages\n",
        "df_one['oe_difficulty'] = [0 if x == 'A1'\n",
        "                   else 2 if x =='A2'\n",
        "                   else 1 if x== 'B1'\n",
        "                   else 3 if x == 'B2'\n",
        "                   else 4 if x == 'C1'\n",
        "                   else 5\n",
        "                   for x in df.difficulty]\n",
        "df.drop(labels='difficulty', axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df_one['sentence'], df_one['oe_difficulty'],\n",
        "                                                                    random_state=2018,     \n",
        "                                                                    test_size=0.1, \n",
        "                                                                    stratify=df_one['oe_difficulty']) \n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.1, \n",
        "                                                                stratify=temp_labels)"
      ],
      "metadata": {
        "id": "FLiKL6NEkYGD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)\n",
        "\n",
        "#The longest sequence message is 265\n",
        "max(seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "zDH3a2vGlSMm",
        "outputId": "d0a96567-0aaf-415a-b65a-48d0d4fdb408"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "265"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "#Tokenization \n",
        "#BERT uses Wordpiece tokenization: \n",
        "#The vocabulary is initialized with all the individual \n",
        "#characters in the language, and then the most frequent/likely combinations \n",
        "#of the existing words in the vocabulary are iteratively added.\n",
        "\n",
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 200,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True)\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 200,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True)\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 200,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHz-Bcevloc7",
        "outputId": "e80b60b8-8457-4b2b-dcc8-2e770b93bfb5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "id": "ZJfqwDq5mi8F"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 1000\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "h585hUQWxllD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "r2B5Y6TBxsEi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        \n",
        "        self.bert = bert \n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "        #Longest sequence = 265\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,265)\n",
        "      \n",
        "        #longest sequence 265 and 6 classes\n",
        "        # dense layer 2 (Output layer)\n",
        "        \n",
        "        self.fc2 = nn.Linear(265,6)\n",
        "\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "        \n",
        "        #pass the inputs to the model  \n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      \n",
        "        x = self.fc1(cls_hs)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "      \n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "u_QzrWA4xvY1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "cUVhhENvx0XS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UYTGvLtx3Fc",
        "outputId": "6bf90d1f-5115-4732-9e53-398259828d33"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes= np.unique(train_labels), y= train_labels)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqepu_WRx6BR",
        "outputId": "f44e6aa2-ac82-4b9d-d37a-240c1aa0d76f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights: [0.98360656 1.00699301 1.00558659 1.00981767 1.00278552 0.99173554]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 3000"
      ],
      "metadata": {
        "id": "HiPZ8E2mCBEa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "## FINE TUNE\n",
        "\n",
        "# function to train the model\n",
        "def train():\n",
        "    \n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "    # empty list to save model predictions\n",
        "    total_preds=[]\n",
        "  \n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "        \n",
        "        # progress update after every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "        \n",
        "        # push the batch to gpu\n",
        "        batch = [r.to(device) for r in batch]\n",
        " \n",
        "        sent_id, mask, labels = batch\n",
        "        \n",
        "        # clear previously calculated gradients \n",
        "        model.zero_grad()        \n",
        "\n",
        "        # get model predictions for the current batch\n",
        "        preds = model(sent_id, mask)\n",
        "\n",
        "        # compute the loss between actual and predicted values\n",
        "        loss = cross_entropy(preds, labels)\n",
        "\n",
        "        # add on to the total loss\n",
        "        total_loss = total_loss + loss.item()\n",
        "\n",
        "        # backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # model predictions are stored on GPU. So, push it to CPU\n",
        "        preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "      # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    #returns the loss and predictions\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "HljvGTgXCHGG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "    \n",
        "    print(\"\\nEvaluating...\")\n",
        "  \n",
        "    # deactivate dropout layers\n",
        "    model.eval()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    \n",
        "    # empty list to save the model predictions\n",
        "    total_preds = []\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "        \n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            \n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [t.to(device) for t in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        # deactivate autograd\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # model predictions\n",
        "            preds = model(sent_id, mask)\n",
        "\n",
        "            # compute the validation loss between actual and predicted values\n",
        "            loss = cross_entropy(preds,labels)\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    # compute the validation loss of the epoch\n",
        "    avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "fO__50MdCK77"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "L6LkXbVoCW9Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21a38b7a-1e0f-4b72-caf2-7d4805bdc690"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Loss: 1.369\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2287 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.379\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2288 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.367\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2289 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2290 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2291 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2292 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2293 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.365\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2294 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.370\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2295 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2296 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2297 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2298 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2299 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.345\n",
            "\n",
            " Epoch 2300 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.365\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2301 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2302 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2303 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2304 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2305 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.365\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2306 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.367\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2307 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.346\n",
            "\n",
            " Epoch 2308 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.364\n",
            "Validation Loss: 1.347\n",
            "\n",
            " Epoch 2309 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.345\n",
            "\n",
            " Epoch 2310 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2311 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2312 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2313 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2314 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2315 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2316 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2317 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.369\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2318 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.368\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2319 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.372\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2320 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2321 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.345\n",
            "\n",
            " Epoch 2322 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.364\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2323 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2324 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2325 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2326 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2327 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2328 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.345\n",
            "\n",
            " Epoch 2329 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.374\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2330 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2331 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.365\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2332 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.367\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2333 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.367\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2334 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2335 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.372\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2336 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2337 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2338 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2339 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2340 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.346\n",
            "\n",
            " Epoch 2341 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2342 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2343 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2344 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2345 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2346 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2347 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2348 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2349 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2350 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.345\n",
            "\n",
            " Epoch 2351 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2352 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2353 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2354 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2355 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.364\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2356 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.345\n",
            "\n",
            " Epoch 2357 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2358 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2359 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2360 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2361 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2362 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.366\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2363 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2364 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.369\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2365 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2366 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.366\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2367 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2368 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2369 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2370 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.366\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2371 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.370\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2372 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2373 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.370\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2374 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2375 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2376 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2377 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2378 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.372\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2379 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2380 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2381 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2382 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.364\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2383 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2384 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2385 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2386 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2387 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2388 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2389 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2390 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2391 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2392 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.344\n",
            "\n",
            " Epoch 2393 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2394 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2395 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2396 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2397 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2398 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2399 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2400 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2401 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2402 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2403 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2404 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2405 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2406 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2407 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2408 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2409 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.367\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2410 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2411 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.368\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2412 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.366\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2413 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2414 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2415 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2416 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.368\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2417 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2418 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2419 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2420 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2421 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2422 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2423 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2424 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2425 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2426 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2427 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.369\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2428 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2429 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2430 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2431 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2432 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.364\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2433 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2434 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2435 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2436 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.365\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2437 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2438 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2439 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2440 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2441 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.366\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2442 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.366\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2443 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2444 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2445 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2446 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2447 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2448 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2449 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.364\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2450 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2451 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2452 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2453 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2454 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2455 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2456 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2457 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2458 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2459 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2460 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2461 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2462 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2463 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2464 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2465 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2466 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.370\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2467 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2468 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2469 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2470 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2471 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2472 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2473 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.342\n",
            "\n",
            " Epoch 2474 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2475 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2476 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2477 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.365\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2478 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2479 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2480 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2481 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2482 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.367\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2483 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.369\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2484 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2485 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2486 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2487 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2488 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2489 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2490 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2491 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2492 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2493 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2494 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2495 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2496 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2497 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2498 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2499 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2500 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2501 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2502 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2503 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2504 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2505 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2506 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2507 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2508 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2509 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2510 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2511 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.343\n",
            "\n",
            " Epoch 2512 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2513 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2514 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2515 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2516 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2517 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2518 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2519 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2520 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2521 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2522 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2523 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2524 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2525 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2526 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2527 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2528 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2529 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2530 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2531 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2532 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2533 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2534 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2535 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2536 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2537 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2538 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2539 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2540 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2541 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2542 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2543 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2544 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.367\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2545 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2546 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2547 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2548 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2549 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2550 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2551 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2552 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2553 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2554 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2555 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.341\n",
            "\n",
            " Epoch 2556 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2557 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2558 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2559 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2560 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2561 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2562 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2563 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2564 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2565 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2566 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2567 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.368\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2568 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2569 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2570 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2571 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2572 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2573 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2574 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2575 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2576 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2577 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2578 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2579 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2580 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2581 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2582 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2583 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2584 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2585 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2586 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2587 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2588 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2589 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2590 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2591 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2592 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2593 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2594 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2595 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2596 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2597 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2598 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2599 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2600 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.364\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2601 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2602 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2603 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2604 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2605 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2606 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2607 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2608 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2609 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2610 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2611 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.340\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2612 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.340\n",
            "\n",
            " Epoch 2613 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2614 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.366\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2615 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2616 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2617 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2618 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2619 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2620 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2621 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2622 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2623 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2624 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2625 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.365\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2626 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2627 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2628 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2629 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2630 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2631 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2632 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2633 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2634 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2635 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2636 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.340\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2637 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2638 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2639 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2640 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2641 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2642 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2643 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2644 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2645 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2646 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2647 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2648 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2649 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.339\n",
            "\n",
            " Epoch 2650 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2651 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2652 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2653 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2654 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2655 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2656 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2657 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2658 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2659 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.336\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2660 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2661 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2662 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2663 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2664 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2665 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2666 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2667 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2668 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2669 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2670 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2671 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2672 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2673 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2674 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2675 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2676 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2677 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2678 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2679 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2680 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2681 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2682 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2683 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2684 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2685 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.340\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2686 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2687 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2688 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2689 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2690 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2691 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2692 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2693 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2694 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2695 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2696 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2697 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2698 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2699 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2700 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2701 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2702 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2703 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2704 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2705 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2706 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2707 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2708 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2709 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.332\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2710 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2711 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.340\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2712 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2713 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2714 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2715 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2716 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2717 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2718 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2719 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2720 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2721 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2722 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2723 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2724 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2725 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2726 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2727 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2728 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2729 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2730 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2731 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2732 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2733 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2734 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2735 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2736 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2737 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2738 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2739 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2740 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2741 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2742 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2743 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2744 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2745 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2746 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.365\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2747 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2748 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2749 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2750 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2751 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2752 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2753 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2754 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2755 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2756 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2757 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2758 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2759 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2760 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2761 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2762 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2763 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2764 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2765 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2766 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2767 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2768 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2769 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.363\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2770 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2771 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2772 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2773 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2774 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.361\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2775 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2776 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2777 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2778 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2779 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2780 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2781 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2782 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2783 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2784 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2785 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2786 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2787 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2788 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2789 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2790 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2791 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2792 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2793 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2794 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2795 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2796 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2797 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2798 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2799 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2800 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2801 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2802 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2803 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2804 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2805 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2806 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2807 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2808 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2809 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.339\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2810 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2811 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.339\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2812 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2813 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2814 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2815 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2816 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2817 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2818 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2819 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2820 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2821 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2822 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2823 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2824 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2825 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2826 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2827 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.333\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2828 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.339\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2829 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2830 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.333\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2831 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2832 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2833 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2834 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2835 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2836 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2837 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2838 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2839 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2840 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2841 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2842 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2843 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2844 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.335\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2845 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2846 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2847 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2848 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2849 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2850 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2851 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.340\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2852 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2853 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2854 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2855 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2856 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2857 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2858 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.334\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2859 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2860 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.332\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2861 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2862 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2863 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2864 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2865 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2866 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.335\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2867 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.337\n",
            "\n",
            " Epoch 2868 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.357\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2869 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.336\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2870 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2871 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2872 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2873 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.336\n",
            "Validation Loss: 1.336\n",
            "\n",
            " Epoch 2874 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2875 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2876 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.340\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2877 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2878 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2879 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2880 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2881 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2882 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2883 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.336\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2884 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2885 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2886 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2887 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2888 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2889 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2890 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2891 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.338\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2892 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2893 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2894 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2895 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2896 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2897 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2898 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2899 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.338\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2900 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.335\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2901 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.332\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2902 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.338\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2903 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2904 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2905 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2906 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2907 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.336\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2908 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2909 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.360\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2910 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2911 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2912 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.338\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2913 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.362\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2914 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2915 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2916 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2917 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.333\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2918 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2919 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2920 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.339\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2921 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2922 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2923 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.336\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2924 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2925 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.358\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2926 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.326\n",
            "\n",
            " Epoch 2927 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2928 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2929 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2930 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.335\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2931 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2932 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2933 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2934 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2935 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.326\n",
            "\n",
            " Epoch 2936 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.339\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2937 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2938 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.336\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2939 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.339\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2940 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2941 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2942 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2943 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2944 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2945 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.338\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2946 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.338\n",
            "\n",
            " Epoch 2947 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2948 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.325\n",
            "\n",
            " Epoch 2949 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.325\n",
            "\n",
            " Epoch 2950 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2951 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2952 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.335\n",
            "\n",
            " Epoch 2953 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.334\n",
            "\n",
            " Epoch 2954 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2955 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.326\n",
            "\n",
            " Epoch 2956 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.338\n",
            "Validation Loss: 1.326\n",
            "\n",
            " Epoch 2957 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2958 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2959 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.352\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2960 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.333\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2961 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2962 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2963 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2964 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.339\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2965 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.340\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2966 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2967 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.355\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2968 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2969 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2970 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.335\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2971 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.338\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2972 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2973 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.337\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2974 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2975 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2976 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.340\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2977 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2978 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.338\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2979 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.346\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2980 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.343\n",
            "Validation Loss: 1.332\n",
            "\n",
            " Epoch 2981 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.359\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2982 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.340\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2983 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2984 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.350\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2985 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.332\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2986 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.331\n",
            "\n",
            " Epoch 2987 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.353\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2988 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.351\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2989 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.349\n",
            "Validation Loss: 1.326\n",
            "\n",
            " Epoch 2990 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 2991 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.334\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2992 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.344\n",
            "Validation Loss: 1.329\n",
            "\n",
            " Epoch 2993 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.354\n",
            "Validation Loss: 1.326\n",
            "\n",
            " Epoch 2994 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.342\n",
            "Validation Loss: 1.324\n",
            "\n",
            " Epoch 2995 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.356\n",
            "Validation Loss: 1.327\n",
            "\n",
            " Epoch 2996 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.347\n",
            "Validation Loss: 1.330\n",
            "\n",
            " Epoch 2997 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.334\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2998 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.333\n",
            "\n",
            " Epoch 2999 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.345\n",
            "Validation Loss: 1.328\n",
            "\n",
            " Epoch 3000 / 3000\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.341\n",
            "Validation Loss: 1.324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BxWEFVXDtmG",
        "outputId": "5d785bda-75d5-4520-e68b-cbace38d7565"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "## HERE WE MAKING PREDICTIONS\n",
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "-MjUxPoSDxfi"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiDm1VFhD1Se",
        "outputId": "d5f97e23-9909-43da-8f29-8aa65c713545"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      1.00      0.84         8\n",
            "           1       0.44      0.50      0.47         8\n",
            "           2       0.67      0.50      0.57         8\n",
            "           3       0.33      0.38      0.35         8\n",
            "           4       0.50      0.50      0.50         8\n",
            "           5       0.60      0.38      0.46         8\n",
            "\n",
            "    accuracy                           0.54        48\n",
            "   macro avg       0.55      0.54      0.53        48\n",
            "weighted avg       0.55      0.54      0.53        48\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import json\n",
        "import os\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "class Prediction:\n",
        "    def __init__(self):\n",
        "        path = 'saved_weights.pt'\n",
        "        checkpoint = torch.load(path,map_location=device)\n",
        "        self.predictor = checkpoint.get(\"model\")\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.tag = checkpoint.get(\"id_map\")\n",
        "\n",
        "    def predict(self,text):\n",
        "        tokens = self.tokenizer.tokenize(text)\n",
        "        tokens = tokens[:max_seq_len - 2]\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_ids = input_ids + [0] * (max_seq_len-len(input_ids))\n",
        "        input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
        "        input_ids = input_ids.to(device)\n",
        "\n",
        "        input_mask = [1]*len(tokens) + [0] * (max_seq_len - len(tokens))\n",
        "        input_mask = torch.tensor(input_mask).unsqueeze(0)\n",
        "        input_mask = input_mask.to(device)\n",
        "\n",
        "        logits = self.predictor(input_ids,input_mask)\n",
        "        prob = torch.nn.functional.softmax(logits,dim=1)\n",
        "        result = [(self.tag[idx],item *100) for idx,item in enumerate(prob[0].tolist())]\n",
        "        preds = logits.detach().cpu().numpy()\n",
        "        pred_val = np.argmax(preds)\n",
        "        pred_val = self.tag[pred_val]\n",
        "        return result,pred_val\n",
        "\n",
        "pred =Prediction()\n",
        "max_seq_len = len(max(df_pred['sentence']))\n",
        "for sentence in df_pred['sentence']:\n",
        "  print(sentence)\n",
        "  print(pred.predict(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "AQLFVozbZC_s",
        "outputId": "9696e58f-6f06-482e-96d4-20c17294bae8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nous dÃ»mes nous excuser des propos que nous eÃ»mes prononcÃ©s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-1d6acac6ba53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-1d6acac6ba53>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0minput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive Bayes \n",
        "0.4822916666666667\n",
        "\n",
        "#Naive bayes with 90% train set\n",
        "0.4979166666666667\n",
        "\n",
        "#SVM\n",
        "#0.4583333333333333\n",
        "\n",
        "\n",
        "#BERT\n",
        "#Batch 32\n",
        "#epoch 10\n",
        "#Accuracy 0.26\n",
        "\n",
        "#Batch 1000\n",
        "#epoch 1000\n",
        "#Accuracy:\n",
        "# 0.32\n",
        "\n",
        "#Batch 1000\n",
        "#epoch100\n",
        "#tokenization 50\n",
        "#0.33 \n",
        "\n",
        "#Batch 10000\n",
        "#epoch 50\n",
        "#tokenization 100\n",
        "#0.35 \n",
        "\n",
        "#Batch 1000\n",
        "#epoch 300\n",
        "#tokenization 150\n",
        "#0.40 \n",
        "\n"
      ],
      "metadata": {
        "id": "3gs9cNeNGj6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test also so this \n",
        "#LR_cv = LogisticRegressionCV(solver='sag', cv=10, max_iter=100, random_state = 0, warm_start= bool,)"
      ],
      "metadata": {
        "id": "sktG_K5RloBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y"
      ],
      "metadata": {
        "id": "efN2pYw6dCuU",
        "outputId": "08ad1660-4205-4966-ce1a-8590bc55c5cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 5, 4, 2, 3, 5, 5, 4, 2, 0, 2, 4, 3, 4, 3, 1, 1, 4, 1, 3, 0, 0, 5,\n",
              "        5, 1, 0, 0, 4, 4, 3, 5, 2, 4, 2, 2, 0, 1, 0, 3, 5, 3, 5, 1, 3, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive bayes test\n",
        "#Encode column\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "#Encoded dataframe\n",
        "df_encoded= df.drop('difficulty', axis = 1)\n",
        "df_encoded\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.1, random_state=0)\n",
        "\n",
        "#Vectorize\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "X_text = text_transformer.transform(X_test)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIa_hocGPcpo",
        "outputId": "48a46793-410d-4e91-bd6b-bcb3b20ff41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.4979166666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST Support Vector Machine\n",
        "#Split \n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.1, random_state=0)\n",
        "\n",
        "#Vectorize\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())\n",
        "\n",
        "#Transform\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "#Split here? \n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "               ])\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = sgd.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "#0.4583333333333333"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCyGMN6cO4qK",
        "outputId": "1a0ac450-a5a2-4b35-9303-c3fc5a411ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.4666666666666667\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Project_guidelines_2022.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}