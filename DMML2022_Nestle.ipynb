{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stergios-Konstantinidis/DMML2022_Nestle/blob/main/DMML2022_Nestle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZKqCFFbNZ04"
      },
      "source": [
        "# Data Mining and Machine Learning - Project\n",
        "\n",
        "## Detecting Difficulty Level of French Texts\n",
        "\n",
        "### Step by step guidelines\n",
        "\n",
        "The following are a set of step by step guidelines to help you get started with your project for the Data Mining and Machine Learning class. \n",
        "To test what you learned in the class, we will hold a competition. You will create a classifier that predicts how the level of some text in French (A1,..., C2). The team with the highest rank will get some goodies in the last class (some souvenirs from tech companies: Amazon, LinkedIn, etc).\n",
        "\n",
        "**2 people per team**\n",
        "\n",
        "Choose a team here:\n",
        "https://moodle.unil.ch/mod/choicegroup/view.php?id=1305831\n",
        "\n",
        "\n",
        "#### 1. ðŸ“‚ Create a public GitHub repository for your team using this naming convention `DMML2022_[your_team_name]` with the following structure:\n",
        "- data (folder) \n",
        "- code (folder) \n",
        "- documentation (folder)\n",
        "- a readme file (.md): *mention team name, participants, brief description of the project, approach, summary of results table and link to the explainatory video (see below).*\n",
        "\n",
        "All team members should contribute to the GitHub repository.\n",
        "\n",
        "#### 2. ðŸ‡° Join the competititon on Kaggle using the invitation link we sent on Slack.\n",
        "\n",
        "Under the Team tab, save your team name (`UNIL_your_team_name`) and make sure your team members join in as well. You can merge your user account with your teammates in order to create a team.\n",
        "\n",
        "#### 3. ðŸ““ Read the data into your colab notebook. There should be one code notebook per team, but all team members can participate and contribute code. \n",
        "\n",
        "You can use either direct the Kaggle API and your Kaggle credentials (as explained below and **entirely optional**), or dowload the data form Kaggle and upload it onto your team's GitHub repository under the data subfolder.\n",
        "\n",
        "#### 4. ðŸ’Ž Train your models and upload the code under your team's GitHub repo. Set the `random_state=0`.\n",
        "- baseline\n",
        "- logistic regression with TFidf vectoriser (simple, no data cleaning)\n",
        "- KNN & hyperparameter optimisation (simple, no data cleaning)\n",
        "- Decision Tree classifier & hyperparameter optimisation (simple, no data cleaning)\n",
        "- Random Forests classifier (simple, no data cleaning)\n",
        "- another technique or combination of techniques of your choice\n",
        "\n",
        "BE CREATIVE! You can use whatever method you want, in order to climb the leaderboard. The only rule is that it must be your own work. Given that, you can use all the online resources you want. \n",
        "\n",
        "#### 5. ðŸŽ¥ Create a YouTube video (10-15 minutes) of your solution and embed it in your notebook. Explain the algorithms used and the evaluation of your solutions. *Select* projects will also be presented live by the group during the last class.\n",
        "\n",
        "\n",
        "### Submission details (one per team)\n",
        "\n",
        "1. Download a ZIPped file of your team's repository and submit it in Moodle here. IMPORTANT: in the comment of the submission, insert a link to the repository on Github.\n",
        "https://moodle.unil.ch/mod/assign/view.php?id=1305833\n",
        "\n",
        "\n",
        "\n",
        "### Grading (one per team)\n",
        "- 20% Kaggle Rank\n",
        "- 50% code quality (using classes, splitting into proper files, documentation, etc)\n",
        "- 15% github quality (include link to video, table with progress over time, organization of code, images, etc)\n",
        "- 15% video quality (good sound, good slides, interesting presentation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-14CAdOoinM"
      },
      "source": [
        "## Some further details for points 3 and 4 above.\n",
        "\n",
        "### 3. Read data into your notebook with the Kaggle API (optional but useful). \n",
        "\n",
        "You can also download the data from Kaggle and put it in your team's repo the data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ_hnJzSNO2g",
        "outputId": "7ad7ed2c-2686-4adc-dcf4-c42b9041b338"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# reading in the data via the Kaggle API\n",
        "\n",
        "# mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJPTz3D7TeQv",
        "outputId": "9817b771-3b02-42d5-8e90-b4d7342e57ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# install Kaggle\n",
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKG1TCddRYTB"
      },
      "source": [
        "### IMPORTANT\n",
        "Log into your Kaggle account, go to Account > API > Create new API token. You will obtain a kaggle.json file. Save it in your Google Drive (not in a folder, in your general drive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JgzLj451YDfV"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrsZLalrSI3u"
      },
      "outputs": [],
      "source": [
        "#read in your Kaggle credentials from Google Drive\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-ETUrrhgdnfU"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BDI60LXKTPzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36494651-0fa5-4f3c-ab5d-094c2bfa91e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading detecting-french-texts-difficulty-level-2022.zip to /content\n",
            "\r  0% 0.00/303k [00:00<?, ?B/s]\n",
            "\r100% 303k/303k [00:00<00:00, 77.4MB/s]\n",
            "Archive:  detecting-french-texts-difficulty-level-2022.zip\n",
            "  inflating: data/sample_submission.csv  \n",
            "  inflating: data/training_data.csv  \n",
            "  inflating: data/unlabelled_test_data.csv  \n"
          ]
        }
      ],
      "source": [
        "# download the dataset from the competition page\n",
        "\n",
        "try:\n",
        "  df = pd.read_csv('/content/data/training_data.csv')\n",
        "except:\n",
        "  !kaggle competitions download -c detecting-french-texts-difficulty-level-2022\n",
        "  !unzip \"detecting-french-texts-difficulty-level-2022.zip\" -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "daqvj7feTx60"
      },
      "outputs": [],
      "source": [
        "# read in your training data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn \n",
        "#import sklearn.model_selection\n",
        "\n",
        "df = pd.read_csv('/content/data/training_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxRSnk5bhTp8"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kpfbtndj0jL"
      },
      "source": [
        "Have a look at the data on which to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7G75Q1gRj49l",
        "outputId": "a16109fb-d0cc-440c-af23-579c703a1b58"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           sentence\n",
              "0   0  Nous dÃ»mes nous excuser des propos que nous eÃ»...\n",
              "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
              "2   2  Et, paradoxalement, boire froid n'est pas la b...\n",
              "3   3  Ce n'est pas Ã©tonnant, car c'est une saison my...\n",
              "4   4  Le corps de Golo lui-mÃªme, d'une essence aussi..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-671735d1-624e-4734-ad22-b51bb95ced37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Nous dÃ»mes nous excuser des propos que nous eÃ»...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Et, paradoxalement, boire froid n'est pas la b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Ce n'est pas Ã©tonnant, car c'est une saison my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Le corps de Golo lui-mÃªme, d'une essence aussi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-671735d1-624e-4734-ad22-b51bb95ced37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-671735d1-624e-4734-ad22-b51bb95ced37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-671735d1-624e-4734-ad22-b51bb95ced37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_pred = pd.read_csv('/content/data/unlabelled_test_data.csv')\n",
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a37hWJ_ckBlk"
      },
      "source": [
        "And this is the format for your submissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gk9H2dLHkFBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c73afe28-2f21-457d-b3c7-f3b55c510efa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id difficulty\n",
              "0   0         A1\n",
              "1   1         A1\n",
              "2   2         A1\n",
              "3   3         A1\n",
              "4   4         A1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dbf4c61d-a673-4048-971d-aa115b94cc92\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbf4c61d-a673-4048-971d-aa115b94cc92')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dbf4c61d-a673-4048-971d-aa115b94cc92 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dbf4c61d-a673-4048-971d-aa115b94cc92');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_example_submission = pd.read_csv('/content/data/sample_submission.csv')\n",
        "df_example_submission.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "import spacy.cli\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
        "import numpy as np\n",
        "from sklearn import linear_model, decomposition, datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmyfGCyIYVTb",
        "outputId": "12968b89-edb8-4e65-8ab8-0815697f96f4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfTgL1erjqQ6"
      },
      "source": [
        "### 4. Train your models\n",
        "\n",
        "Set your X and y variables. \n",
        "Set the `random_state=0`\n",
        "Split the data into a train and test set using the following parameters `train_test_split(X, y, test_size=0.2, random_state=0)`.\n",
        "\n",
        "#### 4.1.Baseline\n",
        "What is the baseline for this classification problem?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVEaPzxuQFxg",
        "outputId": "5eec45f0-892a-4807-9f52-24d442494f08"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvw9eWR4QLM6",
        "outputId": "7a0ae186-d8fb-4fed-bab0-74c59d692abb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t4O_pYiHpiRd"
      },
      "outputs": [],
      "source": [
        "np.random.seed = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WDdFr4xsk5Qf"
      },
      "outputs": [],
      "source": [
        "#Split data set\n",
        "\n",
        "X= df['sentence']\n",
        "y= df['difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlvbPYa0k78l"
      },
      "source": [
        "#### 4.2. Logistic Regression (without data cleaning)\n",
        "\n",
        "Train a simple logistic regression model using a Tfidf vectoriser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eEe3-QNlow4H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "a31e2772-06b2-4ded-90bc-5a8bd86f461a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      000  02h00  03h00   10  100  1000  10000  105   11  110  ...  Ã©vÃ©nement  \\\n",
              "0     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "1     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "2     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "3     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "...   ...    ...    ...  ...  ...   ...    ...  ...  ...  ...  ...        ...   \n",
              "4795  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4796  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4797  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4798  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4799  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "\n",
              "      Ã©vÃ©nements  Ãªtes  Ãªtre  Ãªtres  Ãªut  Ã®le  Ã®les  Ã´ta  Ã´ter  \n",
              "0       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "1       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "2       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "3       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "...          ...   ...   ...    ...  ...  ...   ...  ...   ...  \n",
              "4795    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4796    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4797    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4798    0.200821   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4799    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "\n",
              "[4800 rows x 14585 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2f554a7-2d87-44a7-9afa-62472b9b3aaf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>02h00</th>\n",
              "      <th>03h00</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>10000</th>\n",
              "      <th>105</th>\n",
              "      <th>11</th>\n",
              "      <th>110</th>\n",
              "      <th>...</th>\n",
              "      <th>Ã©vÃ©nement</th>\n",
              "      <th>Ã©vÃ©nements</th>\n",
              "      <th>Ãªtes</th>\n",
              "      <th>Ãªtre</th>\n",
              "      <th>Ãªtres</th>\n",
              "      <th>Ãªut</th>\n",
              "      <th>Ã®le</th>\n",
              "      <th>Ã®les</th>\n",
              "      <th>Ã´ta</th>\n",
              "      <th>Ã´ter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4798</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.200821</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4799</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4800 rows Ã— 14585 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2f554a7-2d87-44a7-9afa-62472b9b3aaf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2f554a7-2d87-44a7-9afa-62472b9b3aaf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2f554a7-2d87-44a7-9afa-62472b9b3aaf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(features.todense(),\n",
        "columns=tfidf.get_feature_names())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "i529gMyojD1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-qO8C5oyov"
      },
      "source": [
        "Calculate accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoder ou label encoder \n",
        "\n",
        "df['oe_difficulty'] = ['0' if x == 'A1'\n",
        "                   else '1' if x =='A2'\n",
        "                   else '2' if x == 'B1'\n",
        "                   else '3' if x=='B2'\n",
        "                   else '4' if x== 'C1'\n",
        "                   else '5'\n",
        "                   for x in df.difficulty]\n",
        "\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)\n",
        "\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "muF_24BsZ1ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PViIQdnDpASy"
      },
      "outputs": [],
      "source": [
        "LR = LogisticRegressionCV(solver='lbfgs', cv=16, max_iter=1000, random_state = 50)\n",
        "\n",
        "LR.fit(X_train_text, y_train)\n",
        "\n",
        "LR.score(X_train_text, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accur_test_LR = LR.score(X_test_text, y_test)\n",
        "accur_test_LR"
      ],
      "metadata": {
        "id": "c1-dsTH4j4fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = LR.predict(X_test_text)\n",
        "precision_score_LR_test =  \"Precision Score : \",precision_score(y_test, y_pred, \n",
        "                                           pos_label='positive',\n",
        "                                           average='micro')\n",
        "recall_score_LR_test = \"Recall Score : \",recall_score(y_test, y_pred, \n",
        "                                           pos_label='positive',\n",
        "                                           average='micro')\n",
        "print(precision_score_LR_test)\n",
        "print(recall_score_LR_test)"
      ],
      "metadata": {
        "id": "hS-CY6xF_mWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for text in X_test_text:\n",
        "  print((X_test.reset_index())._get_value(i, \"sentence\", takeable=False) + \"  \" + LR.predict(text))\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "1gQPHn8pV0pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred = pd.read_csv('/content/data/unlabelled_test_data.csv')\n",
        "df_pred_text = text_transformer.transform(df_pred[\"sentence\"])\n",
        "\n",
        "df_pred['difficulty'] = list(map(lambda x : LR.predict(x)[0], df_pred_text))\n",
        "\n",
        "df_pred = df_pred[[\"id\", \"difficulty\"]]\n",
        "df_pred = df_pred.set_index('id')\n",
        "\n",
        "df_pred.to_csv('file_name.csv')\n",
        "df_pred.head()\n"
      ],
      "metadata": {
        "id": "MghtmHHlURok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D3_dp3apcmr"
      },
      "source": [
        "Have a look at the confusion matrix and identify a few examples of sentences that are not well classified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TTEiuXasNFg"
      },
      "source": [
        "Generate your first predictions on the `unlabelled_test_data.csv`. make sure your predictions match the format of the `unlabelled_test_data.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXG_yIG_pQ8t"
      },
      "source": [
        "#### 4.3. KNN (without data cleaning)\n",
        "\n",
        "Train a KNN classification model using a Tfidf vectoriser. Show the accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPRjD1rSqKKZ"
      },
      "outputs": [],
      "source": [
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoder ou label encoder \n",
        "\n",
        "df['oe_difficulty'] = ['0' if x == 'A1'\n",
        "                   else '1' if x =='A2'\n",
        "                   else '2' if x == 'B1'\n",
        "                   else '3' if x=='B2'\n",
        "                   else '4' if x== 'C1'\n",
        "                   else '5'\n",
        "                   for x in df.difficulty]\n",
        "\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "4y9EVvgqZJSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6rH2Hx0qtB2"
      },
      "source": [
        "Try to improve it by tuning the hyper parameters (`n_neighbors`,   `p`, `weights`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRy18Ce_qxPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5757b6-117f-4630-8930-2cc81dae67d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 19 candidates, totalling 190 fits\n"
          ]
        }
      ],
      "source": [
        "#KNN with 6 neightbors accuracy 0.2073\n",
        "#KNN with 8 Neightbors accuracy 0.2073\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "#K in range from 1 to 8\n",
        "k_range = list(range(1, 20))\n",
        "param_grid = dict(n_neighbors=k_range)\n",
        "\n",
        "# defining parameter range\n",
        "knn_cv = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1)\n",
        "  \n",
        "# fitting the model for grid search\n",
        "knn_cv.fit(X_train_text, y_train)\n",
        "\n",
        "y_pred_knn = knn_cv.predict(X_test_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN with minkowski and algorithm brute give us accuracy of: 0.1719\n",
        "KNeighborsClassifier(algorithm='brute', leaf_size=10, metric='minkowski',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
        "                     weights='uniform')\n",
        "knn.fit(X_train_text, y_train)\n",
        "\n",
        "y_pred_knn = knn.predict(X_test_text)"
      ],
      "metadata": {
        "id": "0PV9gUSySFY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_test = knn.score(X_test_text, y_test)\n",
        "knn_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eFtTuV2ZU0Y",
        "outputId": "3f5d833d-ad39-474e-b79a-d7a2a2f6c094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3229166666666667"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.32291"
      ],
      "metadata": {
        "id": "y1C6ssd9Zu_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#avec micro partout la meme valeur 0.2073\n",
        "#avec macro Precision , recall et Score different\n",
        "def evaluate(test, pred):\n",
        "  precision = precision_score(test, pred, pos_label='positive',\n",
        "                                           average='macro')\n",
        "  recall = recall_score(test, pred, pos_label='positive',\n",
        "                                           average='macro')\n",
        "  f1= f1_score(test, pred, pos_label='positive',\n",
        "                                           average='macro')\n",
        "  print(f\"ACCURACY SCORE:\\n{accuracy_score(test, pred) :.4f}\")\n",
        "  print(f'CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}')\n",
        "\n",
        "evaluate(y_test, y_pred_knn)"
      ],
      "metadata": {
        "id": "Gb7hfqgVXuXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5112e7-0a8c-47b6-f682-07095ada8b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3229\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3927\n",
            "\tRecall: 0.3232\n",
            "\tF1_Score: 0.3076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  average_options = (None, \"micro\", \"macro\", \"weighted\", \"samples\")\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  average_options = (None, \"micro\", \"macro\", \"weighted\", \"samples\")\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  average_options = (None, \"micro\", \"macro\", \"weighted\", \"samples\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFNH1WgNqc62"
      },
      "source": [
        "#### 4.4. Decision Tree Classifier (without data cleaning)\n",
        "\n",
        "Train a Decison Tree classifier, using a Tfidf vectoriser. Show the accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQHjvOp7q11L"
      },
      "source": [
        "Try to improve it by tuning the hyper parameters (`max_depth`, the depth of the decision tree)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Fzl5BUq8JN"
      },
      "outputs": [],
      "source": [
        "# first we tried depth 10, and accuracy of 0.1885\n",
        "# with deph=16 we have accuracy =0.1969\n",
        "# with deph=26 accuracy = 0.2042\n",
        "# wuth deph =40. accuracy=0.2021\n",
        "\n",
        "tree = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                       max_depth=4000, \n",
        "                       random_state=50)\n",
        "tree.fit(X_train_text, y_train)\n",
        "y_pred_tree = tree.predict(X_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(train, pred):\n",
        "  precision = precision_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  recall = recall_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  f1= f1_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  print(f\"ACCURACY SCORE:\\n{accuracy_score(train, pred) :.4f}\")\n",
        "  print(f'CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}')\n",
        "evaluate(y_test, y_pred_tree)"
      ],
      "metadata": {
        "id": "0hGXj6RaaSAU",
        "outputId": "bac47f96-78d7-4a50-ab6d-ed8a32a3c6e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3021\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3021\n",
            "\tRecall: 0.3021\n",
            "\tF1_Score: 0.3021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M52Ys3hcq7ku"
      },
      "source": [
        "#### 4.5. Random Forest Classifier (without data cleaning)\n",
        "\n",
        "Try a Random Forest Classifier, using a Tfidf vectoriser. Show the accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoder ou label encoder \n",
        "\n",
        "df['oe_difficulty'] = ['0' if x == 'A1'\n",
        "                   else '1' if x =='A2'\n",
        "                   else '2' if x == 'B1'\n",
        "                   else '3' if x=='B2'\n",
        "                   else '4' if x== 'C1'\n",
        "                   else '5'\n",
        "                   for x in df.difficulty]\n",
        "\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "hMgVlrBb-wTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())"
      ],
      "metadata": {
        "id": "gw_8yLbsAvuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "6ptuGy7SBE_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "## Doesnt work because it's for continuous varibale like housing price \n",
        "\n",
        "#rf = RandomForestRegressor(random_state = 42)\n",
        "#from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "#print('Parameters currently in use:\\n')\n",
        "#pprint(rf.get_params())"
      ],
      "metadata": {
        "id": "zbvloIiy7coB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "OcJh0i-y7krX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf408f6-5b4e-4378-b4ab-acfa61d5d089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "\n",
        "# Accuracy less than 50\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_clf = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 4, cv = 2, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_clf.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "id": "8XZqDNxD7oFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_test = rf_clf.score(X_test_text, y_test)\n",
        "rf_clf_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug4FBMTZUlAU",
        "outputId": "aa0b1048-335b-42a3-9967-d5c90c24db52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40625"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Marche beaucoup moins bien\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_clf = RandomForestClassifier(criterion='gini',\n",
        "                                 n_estimators=5,\n",
        "                                 random_state=42,\n",
        "                                 n_jobs=20,\n",
        "                                max_depth=40,\n",
        "                                min_samples_split=5,\n",
        "                                max_features=30000,\n",
        "                                bootstrap = bool,\n",
        "                                oob_score= bool,\n",
        "                                warm_start= bool)\n",
        "# Fit the random search model\n",
        "rf_clf.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkEw2RuzUAi7",
        "outputId": "5e2a23e1-818d-4c76-f2b0-bb0034f15dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py:560: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
            "  # axis to be consistent with the classification case and make\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=<class 'bool'>, max_depth=40,\n",
              "                       max_features=30000, min_samples_split=5, n_estimators=5,\n",
              "                       n_jobs=20, oob_score=<class 'bool'>, random_state=42,\n",
              "                       warm_start=<class 'bool'>)"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_test = rf_clf.score(X_test_text, y_test)\n",
        "rf_clf_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ebo8JJYXFyf",
        "outputId": "6ca54cc5-3f40-48a5-ceb4-57f6afbd078f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.325"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "pTPHTA-8C4jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_test = rf_clf.score(X_test_text, y_test)\n",
        "rf_clf_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1jk8RyOBwsN",
        "outputId": "0d19dfff-fc8b-4c99-a499-f2f476111991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40625"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_features, test_labels):\n",
        "    predictions = model.predict(test_features)\n",
        "    errors = abs(predictions - test_labels)\n",
        "    mape = 100 * np.mean(errors / test_labels)\n",
        "    accuracy = 100 - mape\n",
        "    print('Model Performance')\n",
        "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
        "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
        "base_model.fit(X_train_text, y_train)\n",
        "base_accuracy = evaluate(base_model, X_test_text, y_test)\n",
        "\n",
        "\n",
        "best_random = rf_random.best_estimator_\n",
        "random_accuracy = evaluate(best_random, X_test_text, y_test)\n",
        "\n",
        "\n",
        "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n"
      ],
      "metadata": {
        "id": "1BW7KfuTBsvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [80, 90, 100, 110],\n",
        "    'max_features': [2, 3],\n",
        "    'min_samples_leaf': [3, 4, 5],\n",
        "    'min_samples_split': [8, 10, 12],\n",
        "    'n_estimators': [100, 200, 300, 1000]\n",
        "}\n",
        "# Create a based model\n",
        "rf = RandomForestRegressor()\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "grid_search = GridSearchCV(estimator = RandomForestRegressor(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
        "grid_search.fit(X_train_text, y_train)\n",
        "grid_search.best_params_\n",
        "best_grid = grid_search.best_estimator_\n",
        "grid_accuracy = evaluate(best_grid, X_test_text, y_test)\n",
        "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
      ],
      "metadata": {
        "id": "8ANhf-Eg7MlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sssF4NIGrNLa",
        "outputId": "13308591-ea2e-464a-8a62-43adeada6d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-b5306b7fed00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                           \u001b[0;31m#n_informative=2, n_redundant=0,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                           \u001b[0;31m#random_state=0, shuffle=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
          ]
        }
      ],
      "source": [
        "#X, y = make_classification(n_samples=1000, n_features=4,\n",
        "                          #n_informative=2, n_redundant=0,\n",
        "                          #random_state=0, shuffle=False)\n",
        "\n",
        "\n",
        "clf = RandomForestClassifier(random_state=0, bootstrap= True, max_depth= 80,\n",
        " max_features= 3, min_samples_leaf =5, n_estimators= 100)\n",
        "\n",
        "clf.fit(X_train_text, y_train)\n",
        "RandomForestClassifier(...)\n",
        "\n",
        "y_pred_forest =  clf.predict(X_test_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(train, pred):\n",
        "  precision = precision_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  recall = recall_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  f1= f1_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  print(f'CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}')\n",
        "evaluate(y_test, y_pred_forest)"
      ],
      "metadata": {
        "id": "K1F5ZCGgcjeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7329656e-bb39-4fb5-d1b0-2d75391c001e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.1646\n",
            "\tRecall: 0.1646\n",
            "\tF1_Score: 0.1646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_train_text, y_train)\n",
        "accur_train_forest = clf.score(X_train_text, y_train)\n",
        "accur_train_forest"
      ],
      "metadata": {
        "id": "u8qnuSbjaV_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_test_text, y_pred)\n",
        "accur_test_forest = clf.score(X_test_text, y_test)\n",
        "accur_test_forest"
      ],
      "metadata": {
        "id": "xpjEP1YtbCGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-8_3MK1rpZr"
      },
      "source": [
        "#### 4.6. Any other technique, including data cleaning if necessary\n",
        "\n",
        "Try to improve accuracy by training a better model using the techniques seen in class, or combinations of them.\n",
        "\n",
        "As usual, show the accuracy, precision, recall and f1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first let's check if there are NA's\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gIJjicHh2B_",
        "outputId": "9fefab04-6e80-4ffa-ea54-0fd13acec6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id            0\n",
              "sentence      0\n",
              "difficulty    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le_diff = pd.Series(LabelEncoder().fit_transform(df[\"difficulty\"]), name=\"le_diff\")\n",
        "le_diff"
      ],
      "metadata": {
        "id": "jC8R5PMViQDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7360edbf-541b-4f60-ef4c-736bba19ea43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       4\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       2\n",
              "       ..\n",
              "4795    3\n",
              "4796    4\n",
              "4797    1\n",
              "4798    5\n",
              "4799    5\n",
              "Name: le_diff, Length: 4800, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82FvnJycsBFf"
      },
      "source": [
        "#### 4.7. Show a summary of your results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tableau_data = {'Logistic Regression': [0.565376, accur_test_LR, accur_train_LR,  precision_score_LR_train, recall_score_LR_train],\n",
        "        'KNN': [0.565376, accur_test_KNN , accur_train_KNN, precision_score_KNN, recall_score_KNN],\n",
        "        'Decision Tree' : [accur_test_tree, accur_train_tree, precision_score_tree, recall_score_tree]\n",
        "        'Random Forest': [accur_test_forest, accur_train_forest, precision_score_forest, recall_score_forest]}\n",
        "  \n",
        "# Creates pandas DataFrame.\n",
        "tableau_df = pd.DataFrame(tableau_data, index=[ 'Base rate', 'Accuracy Test',\n",
        "                               'Accuracy Train',\n",
        "                               'Precision',\n",
        "                               'Recall'])\n",
        "\n",
        "tableau_df"
      ],
      "metadata": {
        "id": "GtoMd99OU9lp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "300571e6-0725-47a3-afb6-ca41b3ca5758"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-7afa79be8071>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    'Random Forest': [accur_test_forest, accur_train_forest, precision_score_forest, recall_score_forest]}\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.8. Log regression, SVM, Naive Bayes"
      ],
      "metadata": {
        "id": "vIDBOPu4-AHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate the encoder\n",
        "oe=OrdinalEncoder()\n",
        "\n",
        "# set the order of your categories\n",
        "oe.set_params(categories= [['0', '1', '2', '3', '4','5']])\n",
        "\n",
        "# fit-transform a dataframe of the categorical age variable\n",
        "oe_difficulty =oe.fit_transform(df[['oe_difficulty']])\n",
        "\n",
        "#number of values per class\n",
        "oe_difficulty = pd.DataFrame(oe_difficulty).astype('int')\n",
        "oe_difficulty.value_counts()"
      ],
      "metadata": {
        "id": "-8FFD5uDy-fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### NEW TEST\n",
        "\n",
        "#Encode Data\n",
        "#Split data set\n",
        "\n",
        "X= df['sentence']\n",
        "y= df['difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "df['oe_difficulty'] = [0 if x == 'A1'\n",
        "                   else 2 if x =='A2'\n",
        "                   else 1 if x== 'B1'\n",
        "                   else 3 if x == 'B2'\n",
        "                   else 4 if x == 'C1'\n",
        "                   else 5\n",
        "                   for x in df.difficulty]\n",
        "df.drop(labels='difficulty', axis=1)\n",
        "\n",
        "#Encode column\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "#Encoded dataframe\n",
        "df_encoded= df.drop('difficulty', axis = 1)\n",
        "df_encoded\n",
        "\n",
        "#Vectorize\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "6742PIoRdWpd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pca = decomposition.PCA()\n",
        "#std_slc = StandardScaler()\n",
        "#logistic_Reg = linear_model.LogisticRegression()\n",
        "#pipe = Pipeline(steps=[('std_slc', std_slc),\n",
        "                           #('pca', pca),\n",
        "                           #('logistic_Reg', logistic_Reg)])"
      ],
      "metadata": {
        "id": "KiEWpL8BZp4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA\n",
        "#from sklearn.decomposition import PCA\n",
        "#pca = PCA(n_components=2)\n",
        "#pca.fit(X_train_text)\n",
        "#PCA(n_components=2)\n",
        "#print(pca.explained_variance_ratio_)\n",
        "#print(pca.singular_values_)\n"
      ],
      "metadata": {
        "id": "NlTVkN0WbWbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##NEW TEST\n",
        "\n",
        "#Vectorize\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())\n",
        "\n",
        "#Transform\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "#Split \n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCd2BNvXY4DG",
        "outputId": "72ef8fd2-8e08-4418-e93d-c61e9be0285c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR_cv = LogisticRegressionCV(solver='lbfgs', cv=10, max_iter=100, random_state = 0)\n",
        "\n",
        "LR_cv.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "id": "HSwzwt_L0pNm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "acc78ba7-3589-4834-dbb5-34ada38ddee1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a50d9980fe74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mLR_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mLR_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR_accur_test = LR_cv.score(X_test_text, y_test)\n",
        "LR_accur_test"
      ],
      "metadata": {
        "id": "E3qCkfbYgGBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#solver='lbfgs', \n",
        "#cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.5520833333333334\n",
        "\n",
        "#With vectorizer\n",
        "#solver='lbfgs', \n",
        "#cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.5520833333333334\n",
        "\n",
        "\n",
        "#solver='newton-cholesky', \n",
        "#cv=10\n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "#took to much time\n",
        "\n",
        "\n",
        "#solver='lbfgs'\n",
        "# cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.46458333333333335\n",
        "\n",
        "\n",
        "#Vectorize\n",
        "#Encoded Y\n",
        "#solver='lbfgs'\n",
        "# cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.553125\n",
        "\n",
        "#Vectorize\n",
        "#Encoded Y TRUUUUUE\n",
        "#solver='lbfgs'\n",
        "# cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.46458333333333335\n",
        "\n",
        "#LOG reg\n",
        "#Vectorize\n",
        "#Delete columns with intergers\n",
        "#Random_state 5\n",
        "0.471875"
      ],
      "metadata": {
        "id": "9ovRqV_5VeYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW TEST\n",
        "#Vectorize\n",
        "#Drop columns with integer & hours\n",
        "\n",
        "\n",
        "X = pd.DataFrame(features.todense(), columns=tfidf.get_feature_names())\n",
        "y= df['difficulty']\n",
        "\n",
        "X.drop(X.iloc[:, 1:2000], inplace=True, axis=1)\n",
        "\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=13000)\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=5)\n",
        "\n",
        "\n",
        "LR_cv = LogisticRegressionCV(solver='lbfgs', cv=10, max_iter=100, random_state = 0)\n",
        "\n",
        "LR_cv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "NrdMx6FrUEJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_accur_test = LR_cv.score(X_test, y_test)\n",
        "LR_accur_test\n"
      ],
      "metadata": {
        "id": "6PPjWiz7VvBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "254bffb4-5563-4815-f459-117469074b4b"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.471875"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SUBMIT THIS ONE#\n",
        "\n",
        "#TEST Support Vector Machine\n",
        "#Split \n",
        "from sklearn.model_selection import cross_val_score,KFold\n",
        "X= df['sentence']\n",
        "y= df['difficulty']\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.1, random_state=5)\n",
        "\n",
        "#Vectorize\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())\n",
        "\n",
        "#Transform\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "#Split here? \n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "              ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, warm_start= True ,random_state=77777, max_iter=200, class_weight = 'balanced',learning_rate='optimal',validation_fraction = 0.5,tol=None)),\n",
        "               ])\n",
        "#kf=KFold(n_splits=5)\n",
        "\n",
        "\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "y_pred = sgd.predict(X_test)\n",
        "\n",
        "#accuracy 0.5229166666666667#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGD5kWY_9vog",
        "outputId": "d658b7e1-a4dd-4b10-9960-2f991aa70159"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.5229166666666667\n",
            "Cross Validation Scores are [0.45833333 0.45416667 0.45833333 0.44791667 0.43229167]\n",
            "Average Cross Validation score :0.4502083333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy %s' % accuracy_score(y_pred, y_test))"
      ],
      "metadata": {
        "id": "gJFljYHKl6A8",
        "outputId": "fdf01ccb-10f8-43f4-9092-4573c49885c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.5229166666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WORD 2 VEC + LOG REG\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n"
      ],
      "metadata": {
        "id": "TLGKQUshsq0k"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6bnY9RamtRWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive bayes test\n",
        "\n",
        "# NEW TEST NAIVE BAYES\n",
        "# Gaussian Naive Bayes Classification\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import CategoricalNB\n",
        "\n",
        "#Encode column\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "#Encoded dataframe\n",
        "df_encoded= df.drop('difficulty', axis = 1)\n",
        "df_encoded\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.1, random_state=5)\n",
        "\n",
        "#Vectorize\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ] )\n",
        "\n",
        "\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac2FwiKxfc3R",
        "outputId": "a23b22c1-4f34-4baa-cd88-2a472e93c0ed"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.5166666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.9. BERT\n",
        "\n"
      ],
      "metadata": {
        "id": "bH-5SfYm9T5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "##NEW TEST\n",
        "\n",
        "#Vectorize\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())\n",
        "\n",
        "#Transform\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "#Split \n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "ISHYXWcE_0J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "\n",
        "###BERT MODEL\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "#Import bert model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Load the tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "oJQN5uoJXU4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "df_one = df.drop(['difficulty'], axis=1)\n",
        "df_one['oe_difficulty'].value_counts(normalize = True)\n",
        "df_one\n"
      ],
      "metadata": {
        "id": "S_FEeyS8keUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "#lenghts of messages\n",
        "df_one['oe_difficulty'] = [0 if x == 'A1'\n",
        "                   else 2 if x =='A2'\n",
        "                   else 1 if x== 'B1'\n",
        "                   else 3 if x == 'B2'\n",
        "                   else 4 if x == 'C1'\n",
        "                   else 5\n",
        "                   for x in df.difficulty]\n",
        "df.drop(labels='difficulty', axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df_one['sentence'], df_one['oe_difficulty'],\n",
        "                                                                    random_state=2018,     \n",
        "                                                                    test_size=0.1, \n",
        "                                                                    stratify=df_one['oe_difficulty']) \n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.1, \n",
        "                                                                stratify=temp_labels)"
      ],
      "metadata": {
        "id": "FLiKL6NEkYGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)\n",
        "\n",
        "#The longest sequence message is 265\n",
        "max(seq_len)"
      ],
      "metadata": {
        "id": "zDH3a2vGlSMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "#Tokenization \n",
        "#BERT uses Wordpiece tokenization: \n",
        "#The vocabulary is initialized with all the individual \n",
        "#characters in the language, and then the most frequent/likely combinations \n",
        "#of the existing words in the vocabulary are iteratively added.\n",
        "\n",
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 200,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True)\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 200,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True)\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 200,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True)\n"
      ],
      "metadata": {
        "id": "VHz-Bcevloc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "id": "ZJfqwDq5mi8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 1000\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "h585hUQWxllD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "r2B5Y6TBxsEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        \n",
        "        self.bert = bert \n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "        #Longest sequence = 265\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,265)\n",
        "      \n",
        "        #longest sequence 265 and 6 classes\n",
        "        # dense layer 2 (Output layer)\n",
        "        \n",
        "        self.fc2 = nn.Linear(265,6)\n",
        "\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "        \n",
        "        #pass the inputs to the model  \n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      \n",
        "        x = self.fc1(cls_hs)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "      \n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "u_QzrWA4xvY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "cUVhhENvx0XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5) "
      ],
      "metadata": {
        "id": "7UYTGvLtx3Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes= np.unique(train_labels), y= train_labels)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)"
      ],
      "metadata": {
        "id": "Oqepu_WRx6BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 3000"
      ],
      "metadata": {
        "id": "HiPZ8E2mCBEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "## FINE TUNE\n",
        "\n",
        "# function to train the model\n",
        "def train():\n",
        "    \n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "    # empty list to save model predictions\n",
        "    total_preds=[]\n",
        "  \n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "        \n",
        "        # progress update after every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "        \n",
        "        # push the batch to gpu\n",
        "        batch = [r.to(device) for r in batch]\n",
        " \n",
        "        sent_id, mask, labels = batch\n",
        "        \n",
        "        # clear previously calculated gradients \n",
        "        model.zero_grad()        \n",
        "\n",
        "        # get model predictions for the current batch\n",
        "        preds = model(sent_id, mask)\n",
        "\n",
        "        # compute the loss between actual and predicted values\n",
        "        loss = cross_entropy(preds, labels)\n",
        "\n",
        "        # add on to the total loss\n",
        "        total_loss = total_loss + loss.item()\n",
        "\n",
        "        # backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # model predictions are stored on GPU. So, push it to CPU\n",
        "        preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "      # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    #returns the loss and predictions\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "HljvGTgXCHGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "    \n",
        "    print(\"\\nEvaluating...\")\n",
        "  \n",
        "    # deactivate dropout layers\n",
        "    model.eval()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    \n",
        "    # empty list to save the model predictions\n",
        "    total_preds = []\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "        \n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            \n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [t.to(device) for t in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        # deactivate autograd\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # model predictions\n",
        "            preds = model(sent_id, mask)\n",
        "\n",
        "            # compute the validation loss between actual and predicted values\n",
        "            loss = cross_entropy(preds,labels)\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    # compute the validation loss of the epoch\n",
        "    avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "fO__50MdCK77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "id": "L6LkXbVoCW9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "6BxWEFVXDtmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "## HERE WE MAKING PREDICTIONS\n",
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "-MjUxPoSDxfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################### RUN tututututuuu ########################################################\n",
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "id": "EiDm1VFhD1Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#BERT\n",
        "#Batch 32\n",
        "#epoch 10\n",
        "#Accuracy 0.26\n",
        "\n",
        "#Batch 1000\n",
        "#epoch 1000\n",
        "#Accuracy:\n",
        "# 0.32\n",
        "\n",
        "#Batch 1000\n",
        "#epoch100\n",
        "#tokenization 50\n",
        "#0.33 \n",
        "\n",
        "#Batch 10000\n",
        "#epoch 50\n",
        "#tokenization 100\n",
        "#0.35 \n",
        "\n",
        "#Batch 1000\n",
        "#epoch 300\n",
        "#tokenization 150\n",
        "#0.40 \n",
        "\n",
        "#Naive Bayes \n",
        "0.4822916666666667\n",
        "\n",
        "#Naive bayes with 90% train set\n",
        "0.4979166666666667\n",
        "\n",
        "#Naive bayes \n",
        "#Random state: 5\n",
        "#accuracy 0.5166666666666667\n",
        "\n",
        "\n",
        "#SVM\n",
        "#0.4583333333333333\n",
        "\n",
        "#SVM with 90% train set\n",
        "#random state 5\n",
        "#accuracy 0.5270833333333333\n",
        "\n",
        "#SVM with 90% train set\n",
        "#random state 77777\n",
        "#accuracy 0.5229166666666667"
      ],
      "metadata": {
        "id": "3gs9cNeNGj6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_cv = LogisticRegressionCV(solver='sag', cv=10, max_iter=100, random_state = 0, warm_start= bool,)"
      ],
      "metadata": {
        "id": "sktG_K5RloBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic regression 2.0\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "df_ed=pd.DataFrame(features.todense(),columns=tfidf.get_feature_names())\n",
        "df_ed"
      ],
      "metadata": {
        "id": "78WBqGLR8rth"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Project_guidelines_2022.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "gpuClass": "premium",
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}