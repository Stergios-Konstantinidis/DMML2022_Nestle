{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stergios-Konstantinidis/DMML2022_Nestle/blob/main/DMML2022_Nestle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZKqCFFbNZ04"
      },
      "source": [
        "# Data Mining and Machine Learning - Project\n",
        "\n",
        "## Detecting Difficulty Level of French Texts\n",
        "\n",
        "### Step by step guidelines\n",
        "\n",
        "The following are a set of step by step guidelines to help you get started with your project for the Data Mining and Machine Learning class. \n",
        "To test what you learned in the class, we will hold a competition. You will create a classifier that predicts how the level of some text in French (A1,..., C2). The team with the highest rank will get some goodies in the last class (some souvenirs from tech companies: Amazon, LinkedIn, etc).\n",
        "\n",
        "**2 people per team**\n",
        "\n",
        "Choose a team here:\n",
        "https://moodle.unil.ch/mod/choicegroup/view.php?id=1305831\n",
        "\n",
        "\n",
        "#### 1. ðŸ“‚ Create a public GitHub repository for your team using this naming convention `DMML2022_[your_team_name]` with the following structure:\n",
        "- data (folder) \n",
        "- code (folder) \n",
        "- documentation (folder)\n",
        "- a readme file (.md): *mention team name, participants, brief description of the project, approach, summary of results table and link to the explainatory video (see below).*\n",
        "\n",
        "All team members should contribute to the GitHub repository.\n",
        "\n",
        "#### 2. ðŸ‡° Join the competititon on Kaggle using the invitation link we sent on Slack.\n",
        "\n",
        "Under the Team tab, save your team name (`UNIL_your_team_name`) and make sure your team members join in as well. You can merge your user account with your teammates in order to create a team.\n",
        "\n",
        "#### 3. ðŸ““ Read the data into your colab notebook. There should be one code notebook per team, but all team members can participate and contribute code. \n",
        "\n",
        "You can use either direct the Kaggle API and your Kaggle credentials (as explained below and **entirely optional**), or dowload the data form Kaggle and upload it onto your team's GitHub repository under the data subfolder.\n",
        "\n",
        "#### 4. ðŸ’Ž Train your models and upload the code under your team's GitHub repo. Set the `random_state=0`.\n",
        "- baseline\n",
        "- logistic regression with TFidf vectoriser (simple, no data cleaning)\n",
        "- KNN & hyperparameter optimisation (simple, no data cleaning)\n",
        "- Decision Tree classifier & hyperparameter optimisation (simple, no data cleaning)\n",
        "- Random Forests classifier (simple, no data cleaning)\n",
        "- another technique or combination of techniques of your choice\n",
        "\n",
        "BE CREATIVE! You can use whatever method you want, in order to climb the leaderboard. The only rule is that it must be your own work. Given that, you can use all the online resources you want. \n",
        "\n",
        "#### 5. ðŸŽ¥ Create a YouTube video (10-15 minutes) of your solution and embed it in your notebook. Explain the algorithms used and the evaluation of your solutions. *Select* projects will also be presented live by the group during the last class.\n",
        "\n",
        "\n",
        "### Submission details (one per team)\n",
        "\n",
        "1. Download a ZIPped file of your team's repository and submit it in Moodle here. IMPORTANT: in the comment of the submission, insert a link to the repository on Github.\n",
        "https://moodle.unil.ch/mod/assign/view.php?id=1305833\n",
        "\n",
        "\n",
        "\n",
        "### Grading (one per team)\n",
        "- 20% Kaggle Rank\n",
        "- 50% code quality (using classes, splitting into proper files, documentation, etc)\n",
        "- 15% github quality (include link to video, table with progress over time, organization of code, images, etc)\n",
        "- 15% video quality (good sound, good slides, interesting presentation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-14CAdOoinM"
      },
      "source": [
        "## Some further details for points 3 and 4 above.\n",
        "\n",
        "### 3. Read data into your notebook with the Kaggle API (optional but useful). \n",
        "\n",
        "You can also download the data from Kaggle and put it in your team's repo the data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJ_hnJzSNO2g",
        "outputId": "4f17568b-7b18-4a96-9916-ca80ed4cdd24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# reading in the data via the Kaggle API\n",
        "\n",
        "# mount your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJPTz3D7TeQv",
        "outputId": "e2dbe2ad-ab5b-4052-a922-511d4c36ba25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (7.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.64.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2022.12.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "# install Kaggle\n",
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKG1TCddRYTB"
      },
      "source": [
        "### IMPORTANT\n",
        "Log into your Kaggle account, go to Account > API > Create new API token. You will obtain a kaggle.json file. Save it in your Google Drive (not in a folder, in your general drive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JgzLj451YDfV"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KrsZLalrSI3u"
      },
      "outputs": [],
      "source": [
        "#read in your Kaggle credentials from Google Drive\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-ETUrrhgdnfU"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BDI60LXKTPzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29c2acf-10d7-4e89-b49a-15ce36bfa763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading detecting-french-texts-difficulty-level-2022.zip to /content\n",
            "\r  0% 0.00/303k [00:00<?, ?B/s]\n",
            "\r100% 303k/303k [00:00<00:00, 120MB/s]\n",
            "Archive:  detecting-french-texts-difficulty-level-2022.zip\n",
            "  inflating: data/sample_submission.csv  \n",
            "  inflating: data/training_data.csv  \n",
            "  inflating: data/unlabelled_test_data.csv  \n"
          ]
        }
      ],
      "source": [
        "# download the dataset from the competition page\n",
        "\n",
        "try:\n",
        "  df = pd.read_csv('/content/data/training_data.csv')\n",
        "except:\n",
        "  !kaggle competitions download -c detecting-french-texts-difficulty-level-2022\n",
        "  !unzip \"detecting-french-texts-difficulty-level-2022.zip\" -d data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "daqvj7feTx60"
      },
      "outputs": [],
      "source": [
        "# read in your training data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn \n",
        "#import sklearn.model_selection\n",
        "\n",
        "df = pd.read_csv('/content/data/training_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxRSnk5bhTp8"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kpfbtndj0jL"
      },
      "source": [
        "Have a look at the data on which to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "7G75Q1gRj49l",
        "outputId": "f1840ad1-ca8b-42be-e866-8dac85326482"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                           sentence\n",
              "0   0  Nous dÃ»mes nous excuser des propos que nous eÃ»...\n",
              "1   1  Vous ne pouvez pas savoir le plaisir que j'ai ...\n",
              "2   2  Et, paradoxalement, boire froid n'est pas la b...\n",
              "3   3  Ce n'est pas Ã©tonnant, car c'est une saison my...\n",
              "4   4  Le corps de Golo lui-mÃªme, d'une essence aussi..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3d85d84-94a2-44c7-9bb0-acc496bad184\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Nous dÃ»mes nous excuser des propos que nous eÃ»...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Vous ne pouvez pas savoir le plaisir que j'ai ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Et, paradoxalement, boire froid n'est pas la b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Ce n'est pas Ã©tonnant, car c'est une saison my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Le corps de Golo lui-mÃªme, d'une essence aussi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3d85d84-94a2-44c7-9bb0-acc496bad184')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d3d85d84-94a2-44c7-9bb0-acc496bad184 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d3d85d84-94a2-44c7-9bb0-acc496bad184');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df_pred = pd.read_csv('/content/data/unlabelled_test_data.csv')\n",
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a37hWJ_ckBlk"
      },
      "source": [
        "And this is the format for your submissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gk9H2dLHkFBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0669873b-7c89-419d-fa31-616e3c58aa80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id difficulty\n",
              "0   0         A1\n",
              "1   1         A1\n",
              "2   2         A1\n",
              "3   3         A1\n",
              "4   4         A1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2540f088-469a-494b-906e-5fcc3ff48e64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>difficulty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>A1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2540f088-469a-494b-906e-5fcc3ff48e64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2540f088-469a-494b-906e-5fcc3ff48e64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2540f088-469a-494b-906e-5fcc3ff48e64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_example_submission = pd.read_csv('/content/data/sample_submission.csv')\n",
        "df_example_submission.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfTgL1erjqQ6"
      },
      "source": [
        "### 4. Train your models\n",
        "\n",
        "Set your X and y variables. \n",
        "Set the `random_state=0`\n",
        "Split the data into a train and test set using the following parameters `train_test_split(X, y, test_size=0.2, random_state=0)`.\n",
        "\n",
        "#### 4.1.Baseline\n",
        "What is the baseline for this classification problem?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install -U scikit-learn\n",
        "#pip install numpy \n",
        "#pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "1sqqpuY8PZbR",
        "outputId": "ab87b8d5-8e21-4357-d815-2176d93d8290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-79-1c878eef98a6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install -U scikit-learn\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVEaPzxuQFxg",
        "outputId": "64fef25b-7a38-4c41-83ae-25137e054171"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scipy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nvw9eWR4QLM6",
        "outputId": "69093589-e381-47c9-e29f-bf61e1c15954"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "import spacy.cli\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\n",
        "import numpy as np\n",
        "from sklearn import linear_model, decomposition, datasets\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "EXJ03SkaWj4N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t4O_pYiHpiRd"
      },
      "outputs": [],
      "source": [
        "np.random.seed = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WDdFr4xsk5Qf"
      },
      "outputs": [],
      "source": [
        "#Split data set\n",
        "\n",
        "X= df['sentence']\n",
        "y= df['difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlvbPYa0k78l"
      },
      "source": [
        "#### 4.2. Logistic Regression (without data cleaning)\n",
        "\n",
        "Train a simple logistic regression model using a Tfidf vectoriser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eEe3-QNlow4H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "a1d6b49b-e837-4ac4-f378-c1ef7f50e21a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      000  02h00  03h00   10  100  1000  10000  105   11  110  ...  Ã©vÃ©nement  \\\n",
              "0     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "1     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "2     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "3     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4     0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "...   ...    ...    ...  ...  ...   ...    ...  ...  ...  ...  ...        ...   \n",
              "4795  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4796  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4797  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4798  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "4799  0.0    0.0    0.0  0.0  0.0   0.0    0.0  0.0  0.0  0.0  ...        0.0   \n",
              "\n",
              "      Ã©vÃ©nements  Ãªtes  Ãªtre  Ãªtres  Ãªut  Ã®le  Ã®les  Ã´ta  Ã´ter  \n",
              "0       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "1       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "2       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "3       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4       0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "...          ...   ...   ...    ...  ...  ...   ...  ...   ...  \n",
              "4795    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4796    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4797    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4798    0.200821   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "4799    0.000000   0.0   0.0    0.0  0.0  0.0   0.0  0.0   0.0  \n",
              "\n",
              "[4800 rows x 14585 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ce3ad961-1b17-45b2-ace9-47b84f9932dd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>000</th>\n",
              "      <th>02h00</th>\n",
              "      <th>03h00</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>1000</th>\n",
              "      <th>10000</th>\n",
              "      <th>105</th>\n",
              "      <th>11</th>\n",
              "      <th>110</th>\n",
              "      <th>...</th>\n",
              "      <th>Ã©vÃ©nement</th>\n",
              "      <th>Ã©vÃ©nements</th>\n",
              "      <th>Ãªtes</th>\n",
              "      <th>Ãªtre</th>\n",
              "      <th>Ãªtres</th>\n",
              "      <th>Ãªut</th>\n",
              "      <th>Ã®le</th>\n",
              "      <th>Ã®les</th>\n",
              "      <th>Ã´ta</th>\n",
              "      <th>Ã´ter</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4795</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4796</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4797</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4798</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.200821</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4799</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4800 rows Ã— 14585 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce3ad961-1b17-45b2-ace9-47b84f9932dd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ce3ad961-1b17-45b2-ace9-47b84f9932dd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ce3ad961-1b17-45b2-ace9-47b84f9932dd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "i529gMyojD1a"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ-qO8C5oyov"
      },
      "source": [
        "Calculate accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoder ou label encoder \n",
        "\n",
        "df['oe_difficulty'] = ['0' if x == 'A1'\n",
        "                   else '1' if x =='A2'\n",
        "                   else '2' if x == 'B1'\n",
        "                   else '3' if x=='B2'\n",
        "                   else '4' if x== 'C1'\n",
        "                   else '5'\n",
        "                   for x in df.difficulty]\n",
        "\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)\n",
        "\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "muF_24BsZ1ub"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PViIQdnDpASy"
      },
      "outputs": [],
      "source": [
        "LR = LogisticRegressionCV(solver='lbfgs', cv=16, max_iter=1000, random_state = 50)\n",
        "\n",
        "LR.fit(X_train_text, y_train)\n",
        "\n",
        "LR.score(X_train_text, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accur_test_LR = LR.score(X_test_text, y_test)\n",
        "accur_test_LR"
      ],
      "metadata": {
        "id": "c1-dsTH4j4fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = LR.predict(X_test_text)\n",
        "precision_score_LR_test =  \"Precision Score : \",precision_score(y_test, y_pred, \n",
        "                                           pos_label='positive',\n",
        "                                           average='micro')\n",
        "recall_score_LR_test = \"Recall Score : \",recall_score(y_test, y_pred, \n",
        "                                           pos_label='positive',\n",
        "                                           average='micro')\n",
        "print(precision_score_LR_test)\n",
        "print(recall_score_LR_test)"
      ],
      "metadata": {
        "id": "hS-CY6xF_mWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for text in X_test_text:\n",
        "  print((X_test.reset_index())._get_value(i, \"sentence\", takeable=False) + \"  \" + LR.predict(text))\n",
        "  i+=1"
      ],
      "metadata": {
        "id": "1gQPHn8pV0pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred = pd.read_csv('/content/data/unlabelled_test_data.csv')\n",
        "df_pred_text = text_transformer.transform(df_pred[\"sentence\"])\n",
        "\n",
        "df_pred['difficulty'] = list(map(lambda x : LR.predict(x)[0], df_pred_text))\n",
        "\n",
        "df_pred = df_pred[[\"id\", \"difficulty\"]]\n",
        "df_pred = df_pred.set_index('id')\n",
        "\n",
        "df_pred.to_csv('file_name.csv')\n",
        "df_pred.head()\n"
      ],
      "metadata": {
        "id": "MghtmHHlURok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D3_dp3apcmr"
      },
      "source": [
        "Have a look at the confusion matrix and identify a few examples of sentences that are not well classified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TTEiuXasNFg"
      },
      "source": [
        "Generate your first predictions on the `unlabelled_test_data.csv`. make sure your predictions match the format of the `unlabelled_test_data.csv`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXG_yIG_pQ8t"
      },
      "source": [
        "#### 4.3. KNN (without data cleaning)\n",
        "\n",
        "Train a KNN classification model using a Tfidf vectoriser. Show the accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPRjD1rSqKKZ"
      },
      "outputs": [],
      "source": [
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoder ou label encoder \n",
        "\n",
        "df['oe_difficulty'] = ['0' if x == 'A1'\n",
        "                   else '1' if x =='A2'\n",
        "                   else '2' if x == 'B1'\n",
        "                   else '3' if x=='B2'\n",
        "                   else '4' if x== 'C1'\n",
        "                   else '5'\n",
        "                   for x in df.difficulty]\n",
        "\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "4y9EVvgqZJSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6rH2Hx0qtB2"
      },
      "source": [
        "Try to improve it by tuning the hyper parameters (`n_neighbors`,   `p`, `weights`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRy18Ce_qxPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5757b6-117f-4630-8930-2cc81dae67d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 19 candidates, totalling 190 fits\n"
          ]
        }
      ],
      "source": [
        "#KNN with 6 neightbors accuracy 0.2073\n",
        "#KNN with 8 Neightbors accuracy 0.2073\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "#K in range from 1 to 8\n",
        "k_range = list(range(1, 20))\n",
        "param_grid = dict(n_neighbors=k_range)\n",
        "\n",
        "# defining parameter range\n",
        "knn_cv = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', return_train_score=False,verbose=1)\n",
        "  \n",
        "# fitting the model for grid search\n",
        "knn_cv.fit(X_train_text, y_train)\n",
        "\n",
        "y_pred_knn = knn_cv.predict(X_test_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN with minkowski and algorithm brute give us accuracy of: 0.1719\n",
        "KNeighborsClassifier(algorithm='brute', leaf_size=10, metric='minkowski',\n",
        "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
        "                     weights='uniform')\n",
        "knn.fit(X_train_text, y_train)\n",
        "\n",
        "y_pred_knn = knn.predict(X_test_text)"
      ],
      "metadata": {
        "id": "0PV9gUSySFY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_test = knn.score(X_test_text, y_test)\n",
        "knn_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eFtTuV2ZU0Y",
        "outputId": "3f5d833d-ad39-474e-b79a-d7a2a2f6c094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3229166666666667"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.32291"
      ],
      "metadata": {
        "id": "y1C6ssd9Zu_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#avec micro partout la meme valeur 0.2073\n",
        "#avec macro Precision , recall et Score different\n",
        "def evaluate(test, pred):\n",
        "  precision = precision_score(test, pred, pos_label='positive',\n",
        "                                           average='macro')\n",
        "  recall = recall_score(test, pred, pos_label='positive',\n",
        "                                           average='macro')\n",
        "  f1= f1_score(test, pred, pos_label='positive',\n",
        "                                           average='macro')\n",
        "  print(f\"ACCURACY SCORE:\\n{accuracy_score(test, pred) :.4f}\")\n",
        "  print(f'CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}')\n",
        "\n",
        "evaluate(y_test, y_pred_knn)"
      ],
      "metadata": {
        "id": "Gb7hfqgVXuXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf5112e7-0a8c-47b6-f682-07095ada8b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3229\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3927\n",
            "\tRecall: 0.3232\n",
            "\tF1_Score: 0.3076\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  average_options = (None, \"micro\", \"macro\", \"weighted\", \"samples\")\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  average_options = (None, \"micro\", \"macro\", \"weighted\", \"samples\")\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  average_options = (None, \"micro\", \"macro\", \"weighted\", \"samples\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFNH1WgNqc62"
      },
      "source": [
        "#### 4.4. Decision Tree Classifier (without data cleaning)\n",
        "\n",
        "Train a Decison Tree classifier, using a Tfidf vectoriser. Show the accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQHjvOp7q11L"
      },
      "source": [
        "Try to improve it by tuning the hyper parameters (`max_depth`, the depth of the decision tree)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Fzl5BUq8JN"
      },
      "outputs": [],
      "source": [
        "# first we tried depth 10, and accuracy of 0.1885\n",
        "# with deph=16 we have accuracy =0.1969\n",
        "# with deph=26 accuracy = 0.2042\n",
        "# wuth deph =40. accuracy=0.2021\n",
        "\n",
        "tree = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
        "                       max_depth=4000, \n",
        "                       random_state=50)\n",
        "tree.fit(X_train_text, y_train)\n",
        "y_pred_tree = tree.predict(X_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(train, pred):\n",
        "  precision = precision_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  recall = recall_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  f1= f1_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  print(f\"ACCURACY SCORE:\\n{accuracy_score(train, pred) :.4f}\")\n",
        "  print(f'CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}')\n",
        "evaluate(y_test, y_pred_tree)"
      ],
      "metadata": {
        "id": "0hGXj6RaaSAU",
        "outputId": "bac47f96-78d7-4a50-ab6d-ed8a32a3c6e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ACCURACY SCORE:\n",
            "0.3021\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.3021\n",
            "\tRecall: 0.3021\n",
            "\tF1_Score: 0.3021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1370: UserWarning: Note that pos_label (set to 'positive') is ignored when average != 'binary' (got 'micro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M52Ys3hcq7ku"
      },
      "source": [
        "#### 4.5. Random Forest Classifier (without data cleaning)\n",
        "\n",
        "Try a Random Forest Classifier, using a Tfidf vectoriser. Show the accuracy, precision, recall and F1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#One hot encoder ou label encoder \n",
        "\n",
        "df['oe_difficulty'] = ['0' if x == 'A1'\n",
        "                   else '1' if x =='A2'\n",
        "                   else '2' if x == 'B1'\n",
        "                   else '3' if x=='B2'\n",
        "                   else '4' if x== 'C1'\n",
        "                   else '5'\n",
        "                   for x in df.difficulty]\n",
        "\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "hMgVlrBb-wTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())"
      ],
      "metadata": {
        "id": "gw_8yLbsAvuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "6ptuGy7SBE_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "## Doesnt work because it's for continuous varibale like housing price \n",
        "\n",
        "#rf = RandomForestRegressor(random_state = 42)\n",
        "#from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "#print('Parameters currently in use:\\n')\n",
        "#pprint(rf.get_params())"
      ],
      "metadata": {
        "id": "zbvloIiy7coB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)"
      ],
      "metadata": {
        "id": "OcJh0i-y7krX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf408f6-5b4e-4378-b4ab-acfa61d5d089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "\n",
        "# Accuracy less than 50\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_clf = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 4, cv = 2, verbose=1, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_clf.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "id": "8XZqDNxD7oFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_test = rf_clf.score(X_test_text, y_test)\n",
        "rf_clf_test\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug4FBMTZUlAU",
        "outputId": "aa0b1048-335b-42a3-9967-d5c90c24db52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40625"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##Marche beaucoup moins bien\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_clf = RandomForestClassifier(criterion='gini',\n",
        "                                 n_estimators=5,\n",
        "                                 random_state=42,\n",
        "                                 n_jobs=20,\n",
        "                                max_depth=40,\n",
        "                                min_samples_split=5,\n",
        "                                max_features=30000,\n",
        "                                bootstrap = bool,\n",
        "                                oob_score= bool,\n",
        "                                warm_start= bool)\n",
        "# Fit the random search model\n",
        "rf_clf.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkEw2RuzUAi7",
        "outputId": "5e2a23e1-818d-4c76-f2b0-bb0034f15dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_forest.py:560: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
            "  # axis to be consistent with the classification case and make\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=<class 'bool'>, max_depth=40,\n",
              "                       max_features=30000, min_samples_split=5, n_estimators=5,\n",
              "                       n_jobs=20, oob_score=<class 'bool'>, random_state=42,\n",
              "                       warm_start=<class 'bool'>)"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_test = rf_clf.score(X_test_text, y_test)\n",
        "rf_clf_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ebo8JJYXFyf",
        "outputId": "6ca54cc5-3f40-48a5-ceb4-57f6afbd078f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.325"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "pTPHTA-8C4jR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf_test = rf_clf.score(X_test_text, y_test)\n",
        "rf_clf_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1jk8RyOBwsN",
        "outputId": "0d19dfff-fc8b-4c99-a499-f2f476111991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40625"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_features, test_labels):\n",
        "    predictions = model.predict(test_features)\n",
        "    errors = abs(predictions - test_labels)\n",
        "    mape = 100 * np.mean(errors / test_labels)\n",
        "    accuracy = 100 - mape\n",
        "    print('Model Performance')\n",
        "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
        "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
        "    \n",
        "    return accuracy\n",
        "\n",
        "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
        "base_model.fit(X_train_text, y_train)\n",
        "base_accuracy = evaluate(base_model, X_test_text, y_test)\n",
        "\n",
        "\n",
        "best_random = rf_random.best_estimator_\n",
        "random_accuracy = evaluate(best_random, X_test_text, y_test)\n",
        "\n",
        "\n",
        "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n"
      ],
      "metadata": {
        "id": "1BW7KfuTBsvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [80, 90, 100, 110],\n",
        "    'max_features': [2, 3],\n",
        "    'min_samples_leaf': [3, 4, 5],\n",
        "    'min_samples_split': [8, 10, 12],\n",
        "    'n_estimators': [100, 200, 300, 1000]\n",
        "}\n",
        "# Create a based model\n",
        "rf = RandomForestRegressor()\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "grid_search = GridSearchCV(estimator = RandomForestRegressor(), param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
        "grid_search.fit(X_train_text, y_train)\n",
        "grid_search.best_params_\n",
        "best_grid = grid_search.best_estimator_\n",
        "grid_accuracy = evaluate(best_grid, X_test_text, y_test)\n",
        "print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))"
      ],
      "metadata": {
        "id": "8ANhf-Eg7MlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sssF4NIGrNLa",
        "outputId": "13308591-ea2e-464a-8a62-43adeada6d57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-b5306b7fed00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                           \u001b[0;31m#n_informative=2, n_redundant=0,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                           \u001b[0;31m#random_state=0, shuffle=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
          ]
        }
      ],
      "source": [
        "#X, y = make_classification(n_samples=1000, n_features=4,\n",
        "                          #n_informative=2, n_redundant=0,\n",
        "                          #random_state=0, shuffle=False)\n",
        "\n",
        "\n",
        "clf = RandomForestClassifier(random_state=0, bootstrap= True, max_depth= 80,\n",
        " max_features= 3, min_samples_leaf =5, n_estimators= 100)\n",
        "\n",
        "clf.fit(X_train_text, y_train)\n",
        "RandomForestClassifier(...)\n",
        "\n",
        "y_pred_forest =  clf.predict(X_test_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(train, pred):\n",
        "  precision = precision_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  recall = recall_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  f1= f1_score(train, pred,pos_label='positive',\n",
        "                                           average='micro')\n",
        "  print(f'CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}')\n",
        "evaluate(y_test, y_pred_forest)"
      ],
      "metadata": {
        "id": "K1F5ZCGgcjeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7329656e-bb39-4fb5-d1b0-2d75391c001e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.1646\n",
            "\tRecall: 0.1646\n",
            "\tF1_Score: 0.1646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_train_text, y_train)\n",
        "accur_train_forest = clf.score(X_train_text, y_train)\n",
        "accur_train_forest"
      ],
      "metadata": {
        "id": "u8qnuSbjaV_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(X_test_text, y_pred)\n",
        "accur_test_forest = clf.score(X_test_text, y_test)\n",
        "accur_test_forest"
      ],
      "metadata": {
        "id": "xpjEP1YtbCGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-8_3MK1rpZr"
      },
      "source": [
        "#### 4.6. Any other technique, including data cleaning if necessary\n",
        "\n",
        "Try to improve accuracy by training a better model using the techniques seen in class, or combinations of them.\n",
        "\n",
        "As usual, show the accuracy, precision, recall and f1 score on the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first let's check if there are NA's\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gIJjicHh2B_",
        "outputId": "9fefab04-6e80-4ffa-ea54-0fd13acec6e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id            0\n",
              "sentence      0\n",
              "difficulty    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "le_diff = pd.Series(LabelEncoder().fit_transform(df[\"difficulty\"]), name=\"le_diff\")\n",
        "le_diff"
      ],
      "metadata": {
        "id": "jC8R5PMViQDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7360edbf-541b-4f60-ef4c-736bba19ea43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       4\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       2\n",
              "       ..\n",
              "4795    3\n",
              "4796    4\n",
              "4797    1\n",
              "4798    5\n",
              "4799    5\n",
              "Name: le_diff, Length: 4800, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82FvnJycsBFf"
      },
      "source": [
        "#### 4.7. Show a summary of your results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tableau_data = {'Logistic Regression': [0.565376, accur_test_LR, accur_train_LR,  precision_score_LR_train, recall_score_LR_train],\n",
        "        'KNN': [0.565376, accur_test_KNN , accur_train_KNN, precision_score_KNN, recall_score_KNN],\n",
        "        'Decision Tree' : [accur_test_tree, accur_train_tree, precision_score_tree, recall_score_tree]\n",
        "        'Random Forest': [accur_test_forest, accur_train_forest, precision_score_forest, recall_score_forest]}\n",
        "  \n",
        "# Creates pandas DataFrame.\n",
        "tableau_df = pd.DataFrame(tableau_data, index=[ 'Base rate', 'Accuracy Test',\n",
        "                               'Accuracy Train',\n",
        "                               'Precision',\n",
        "                               'Recall'])\n",
        "\n",
        "tableau_df"
      ],
      "metadata": {
        "id": "GtoMd99OU9lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate the encoder\n",
        "oe=OrdinalEncoder()\n",
        "\n",
        "# set the order of your categories\n",
        "oe.set_params(categories= [['0', '1', '2', '3', '4','5']])\n",
        "\n",
        "# fit-transform a dataframe of the categorical age variable\n",
        "oe_difficulty =oe.fit_transform(df[['oe_difficulty']])\n",
        "\n",
        "#number of values per class\n",
        "oe_difficulty = pd.DataFrame(oe_difficulty).astype('int')\n",
        "oe_difficulty.value_counts()"
      ],
      "metadata": {
        "id": "-8FFD5uDy-fv",
        "outputId": "f650ada9-5fc8-48d1-e023-f3607cb23ce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    813\n",
              "5    807\n",
              "4    798\n",
              "1    795\n",
              "2    795\n",
              "3    792\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### NEW TEST\n",
        "\n",
        "#Encode Data\n",
        "df['oe_difficulty'] = [0 if x == 'A1'\n",
        "                   else 2 if x =='A2'\n",
        "                   else 1 if x== 'B1'\n",
        "                   else 3 if x == 'B2'\n",
        "                   else 4 if x == 'C1'\n",
        "                   else 5\n",
        "                   for x in df.difficulty]\n",
        "df.drop(labels='difficulty', axis=1)\n",
        "\n",
        "#Encode column\n",
        "df.oe_difficulty.value_counts()\n",
        "newY = df['oe_difficulty']\n",
        "X= df['sentence']\n",
        "newY = df['oe_difficulty']\n",
        "\n",
        "#Encoded dataframe\n",
        "df_encoded= df.drop('difficulty', axis = 1)\n",
        "df_encoded\n",
        "\n",
        "#Vectorize\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, newY, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "6742PIoRdWpd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pca = decomposition.PCA()\n",
        "#std_slc = StandardScaler()\n",
        "#logistic_Reg = linear_model.LogisticRegression()\n",
        "#pipe = Pipeline(steps=[('std_slc', std_slc),\n",
        "                           #('pca', pca),\n",
        "                           #('logistic_Reg', logistic_Reg)])"
      ],
      "metadata": {
        "id": "KiEWpL8BZp4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA\n",
        "#from sklearn.decomposition import PCA\n",
        "#pca = PCA(n_components=2)\n",
        "#pca.fit(X_train_text)\n",
        "#PCA(n_components=2)\n",
        "#print(pca.explained_variance_ratio_)\n",
        "#print(pca.singular_values_)\n"
      ],
      "metadata": {
        "id": "NlTVkN0WbWbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##NEW TEST\n",
        "\n",
        "#Vectorize\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())\n",
        "\n",
        "#Transform\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "#Split \n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCd2BNvXY4DG",
        "outputId": "8941b90a-489c-47a6-c6f7-69fa0102a009"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR_cv = LogisticRegressionCV(solver='lbfgs', cv=10, max_iter=100, random_state = 0)\n",
        "\n",
        "LR_cv.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "id": "HSwzwt_L0pNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_accur_test = LR_cv.score(X_test_text, y_test)\n",
        "LR_accur_test"
      ],
      "metadata": {
        "id": "E3qCkfbYgGBz",
        "outputId": "de2fde0c-7127-4e57-b605-aa411303a8db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.46458333333333335"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#solver='lbfgs', \n",
        "#cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.5520833333333334\n",
        "\n",
        "#With vectorizer\n",
        "#solver='lbfgs', \n",
        "#cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.5520833333333334\n",
        "\n",
        "\n",
        "#solver='newton-cholesky', \n",
        "#cv=10\n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "#took to much time\n",
        "\n",
        "\n",
        "#solver='lbfgs'\n",
        "# cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.46458333333333335\n",
        "\n",
        "\n",
        "#Vectorize\n",
        "#Encoded Y\n",
        "#solver='lbfgs'\n",
        "# cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.553125\n",
        "\n",
        "#Vectorize\n",
        "#Encoded Y TRUUUUUE\n",
        "#solver='lbfgs'\n",
        "# cv=10, \n",
        "#max_iter=100, \n",
        "#random_state = 0)\n",
        "0.46458333333333335"
      ],
      "metadata": {
        "id": "9ovRqV_5VeYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fcbe2f-04a6-4bac-c926-0135880d4bc1"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.553125"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW TEST\n",
        "\n",
        "LR_cv = LogisticRegressionCV(solver='newton-cholesky', cv=10, max_iter=100, random_state = 0)\n",
        "\n",
        "LR_cv.fit(X_train_text, y_train)"
      ],
      "metadata": {
        "id": "NrdMx6FrUEJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_accur_test = LR_cv.score(X_test_text, y_test)\n",
        "LR_accur_test"
      ],
      "metadata": {
        "id": "6PPjWiz7VvBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###BERT MODEL\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast\n",
        "\n",
        "# specify GPU\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "#Import bert model\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#Load the tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "oJQN5uoJXU4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_one = df.drop(['difficulty'], axis=1)\n",
        "df_one['oe_difficulty'].value_counts(normalize = True)\n",
        "df_one\n"
      ],
      "metadata": {
        "id": "S_FEeyS8keUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lenghts of messages\n",
        "df_one['oe_difficulty'] = [0 if x == 'A1'\n",
        "                   else 2 if x =='A2'\n",
        "                   else 1 if x== 'B1'\n",
        "                   else 3 if x == 'B2'\n",
        "                   else 4 if x == 'C1'\n",
        "                   else 5\n",
        "                   for x in df.difficulty]\n",
        "df.drop(labels='difficulty', axis=1)\n",
        "\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df_one['sentence'], df_one['oe_difficulty'],\n",
        "                                                                    random_state=2018,     \n",
        "                                                                    test_size=0.1, \n",
        "                                                                    stratify=df_one['oe_difficulty']) \n",
        "\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
        "                                                                random_state=2018, \n",
        "                                                                test_size=0.1, \n",
        "                                                                stratify=temp_labels)"
      ],
      "metadata": {
        "id": "FLiKL6NEkYGD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in train_text]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)\n",
        "\n",
        "#The longest sequence message is 265\n",
        "max(seq_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "zDH3a2vGlSMm",
        "outputId": "85e5c54f-9b0c-469a-c229-bf8a5087362b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "265"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization \n",
        "#BERT uses Wordpiece tokenization: \n",
        "#The vocabulary is initialized with all the individual \n",
        "#characters in the language, and then the most frequent/likely combinations \n",
        "#of the existing words in the vocabulary are iteratively added.\n",
        "\n",
        "# tokenize and encode sequences in the training set\n",
        "tokens_train = tokenizer.batch_encode_plus(\n",
        "    train_text.tolist(),\n",
        "    max_length = 200,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True)\n",
        "\n",
        "# tokenize and encode sequences in the validation set\n",
        "tokens_val = tokenizer.batch_encode_plus(\n",
        "    val_text.tolist(),\n",
        "    max_length = 200,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True)\n",
        "\n",
        "# tokenize and encode sequences in the test set\n",
        "tokens_test = tokenizer.batch_encode_plus(\n",
        "    test_text.tolist(),\n",
        "    max_length = 200,\n",
        "    pad_to_max_length=True,\n",
        "    truncation=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHz-Bcevloc7",
        "outputId": "58a6a6b0-3dec-414e-e333-e41b1906b954"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## convert lists to tensors\n",
        "\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
        "train_y = torch.tensor(train_labels.tolist())\n",
        "\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
        "val_y = torch.tensor(val_labels.tolist())\n",
        "\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "metadata": {
        "id": "ZJfqwDq5mi8F"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "#define a batch size\n",
        "batch_size = 1000\n",
        "\n",
        "# wrap tensors\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "train_sampler = RandomSampler(train_data)\n",
        "\n",
        "# dataLoader for train set\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# wrap tensors\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
        "\n",
        "# sampler for sampling the data during training\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "\n",
        "# dataLoader for validation set\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "h585hUQWxllD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze all the parameters\n",
        "for param in bert.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "r2B5Y6TBxsEi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT_Arch(nn.Module):\n",
        "\n",
        "    def __init__(self, bert):\n",
        "        super(BERT_Arch, self).__init__()\n",
        "        \n",
        "        self.bert = bert \n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "      \n",
        "        # relu activation function\n",
        "        self.relu =  nn.ReLU()\n",
        "\n",
        "        #Longest sequence = 265\n",
        "        # dense layer 1\n",
        "        self.fc1 = nn.Linear(768,265)\n",
        "      \n",
        "        #longest sequence 265 and 6 classes\n",
        "        # dense layer 2 (Output layer)\n",
        "        \n",
        "        self.fc2 = nn.Linear(265,6)\n",
        "\n",
        "        #softmax activation function\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    #define the forward pass\n",
        "    def forward(self, sent_id, mask):\n",
        "        \n",
        "        #pass the inputs to the model  \n",
        "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
        "      \n",
        "        x = self.fc1(cls_hs)\n",
        "\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # output layer\n",
        "        x = self.fc2(x)\n",
        "      \n",
        "        # apply softmax activation\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "u_QzrWA4xvY1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the pre-trained BERT to our define architecture\n",
        "model = BERT_Arch(bert)\n",
        "\n",
        "# push the model to GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "cUVhhENvx0XS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer from hugging face transformers\n",
        "from transformers import AdamW\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = AdamW(model.parameters(),lr = 1e-5) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UYTGvLtx3Fc",
        "outputId": "2619f025-c4bf-49bf-c0d9-0783c2480601"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "#compute the class weights\n",
        "\n",
        "class_weights = compute_class_weight('balanced', classes= np.unique(train_labels), y= train_labels)\n",
        "\n",
        "print(\"Class Weights:\",class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqepu_WRx6BR",
        "outputId": "fc989026-8bf0-4c8c-bd84-9617c5b5e904"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights: [0.98360656 1.00699301 1.00558659 1.00981767 1.00278552 0.99173554]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting list of class weights to a tensor\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "weights = weights.to(device)\n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) \n",
        "\n",
        "# number of training epochs\n",
        "epochs = 300"
      ],
      "metadata": {
        "id": "HiPZ8E2mCBEa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FINE TUNE\n",
        "\n",
        "# function to train the model\n",
        "def train():\n",
        "    \n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "  \n",
        "    # empty list to save model predictions\n",
        "    total_preds=[]\n",
        "  \n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(train_dataloader):\n",
        "        \n",
        "        # progress update after every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "        \n",
        "        # push the batch to gpu\n",
        "        batch = [r.to(device) for r in batch]\n",
        " \n",
        "        sent_id, mask, labels = batch\n",
        "        \n",
        "        # clear previously calculated gradients \n",
        "        model.zero_grad()        \n",
        "\n",
        "        # get model predictions for the current batch\n",
        "        preds = model(sent_id, mask)\n",
        "\n",
        "        # compute the loss between actual and predicted values\n",
        "        loss = cross_entropy(preds, labels)\n",
        "\n",
        "        # add on to the total loss\n",
        "        total_loss = total_loss + loss.item()\n",
        "\n",
        "        # backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # model predictions are stored on GPU. So, push it to CPU\n",
        "        preds=preds.detach().cpu().numpy()\n",
        "\n",
        "    # append the model predictions\n",
        "    total_preds.append(preds)\n",
        "\n",
        "    # compute the training loss of the epoch\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
        "      # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    #returns the loss and predictions\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "HljvGTgXCHGG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the model\n",
        "def evaluate():\n",
        "    \n",
        "    print(\"\\nEvaluating...\")\n",
        "  \n",
        "    # deactivate dropout layers\n",
        "    model.eval()\n",
        "\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    \n",
        "    # empty list to save the model predictions\n",
        "    total_preds = []\n",
        "\n",
        "    # iterate over batches\n",
        "    for step,batch in enumerate(val_dataloader):\n",
        "        \n",
        "        # Progress update every 50 batches.\n",
        "        if step % 50 == 0 and not step == 0:\n",
        "            \n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "\n",
        "        # push the batch to gpu\n",
        "        batch = [t.to(device) for t in batch]\n",
        "\n",
        "        sent_id, mask, labels = batch\n",
        "\n",
        "        # deactivate autograd\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            # model predictions\n",
        "            preds = model(sent_id, mask)\n",
        "\n",
        "            # compute the validation loss between actual and predicted values\n",
        "            loss = cross_entropy(preds,labels)\n",
        "\n",
        "            total_loss = total_loss + loss.item()\n",
        "\n",
        "            preds = preds.detach().cpu().numpy()\n",
        "\n",
        "            total_preds.append(preds)\n",
        "\n",
        "    # compute the validation loss of the epoch\n",
        "    avg_loss = total_loss / len(val_dataloader) \n",
        "\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\n",
        "    total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "    return avg_loss, total_preds"
      ],
      "metadata": {
        "id": "fO__50MdCK77"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "#for each epoch\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    \n",
        "    #train model\n",
        "    train_loss, _ = train()\n",
        "    \n",
        "    #evaluate model\n",
        "    valid_loss, _ = evaluate()\n",
        "    \n",
        "    #save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    # append training and validation loss\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6LkXbVoCW9Q",
        "outputId": "26943dd4-6293-442b-bd0a-1bd675db2757"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.802\n",
            "Validation Loss: 1.802\n",
            "\n",
            " Epoch 2 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.797\n",
            "Validation Loss: 1.796\n",
            "\n",
            " Epoch 3 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.793\n",
            "Validation Loss: 1.791\n",
            "\n",
            " Epoch 4 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.790\n",
            "Validation Loss: 1.787\n",
            "\n",
            " Epoch 5 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.787\n",
            "Validation Loss: 1.784\n",
            "\n",
            " Epoch 6 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.783\n",
            "Validation Loss: 1.780\n",
            "\n",
            " Epoch 7 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.780\n",
            "Validation Loss: 1.778\n",
            "\n",
            " Epoch 8 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.778\n",
            "Validation Loss: 1.775\n",
            "\n",
            " Epoch 9 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.776\n",
            "Validation Loss: 1.773\n",
            "\n",
            " Epoch 10 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.774\n",
            "Validation Loss: 1.771\n",
            "\n",
            " Epoch 11 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.771\n",
            "Validation Loss: 1.769\n",
            "\n",
            " Epoch 12 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.771\n",
            "Validation Loss: 1.767\n",
            "\n",
            " Epoch 13 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.767\n",
            "Validation Loss: 1.765\n",
            "\n",
            " Epoch 14 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.767\n",
            "Validation Loss: 1.764\n",
            "\n",
            " Epoch 15 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.765\n",
            "Validation Loss: 1.762\n",
            "\n",
            " Epoch 16 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.765\n",
            "Validation Loss: 1.760\n",
            "\n",
            " Epoch 17 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.761\n",
            "Validation Loss: 1.758\n",
            "\n",
            " Epoch 18 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.759\n",
            "Validation Loss: 1.756\n",
            "\n",
            " Epoch 19 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.758\n",
            "Validation Loss: 1.754\n",
            "\n",
            " Epoch 20 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.758\n",
            "Validation Loss: 1.752\n",
            "\n",
            " Epoch 21 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.754\n",
            "Validation Loss: 1.750\n",
            "\n",
            " Epoch 22 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.751\n",
            "Validation Loss: 1.748\n",
            "\n",
            " Epoch 23 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.750\n",
            "Validation Loss: 1.746\n",
            "\n",
            " Epoch 24 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.747\n",
            "Validation Loss: 1.744\n",
            "\n",
            " Epoch 25 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.748\n",
            "Validation Loss: 1.742\n",
            "\n",
            " Epoch 26 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.745\n",
            "Validation Loss: 1.740\n",
            "\n",
            " Epoch 27 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.744\n",
            "Validation Loss: 1.738\n",
            "\n",
            " Epoch 28 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.744\n",
            "Validation Loss: 1.736\n",
            "\n",
            " Epoch 29 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.739\n",
            "Validation Loss: 1.735\n",
            "\n",
            " Epoch 30 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.738\n",
            "Validation Loss: 1.733\n",
            "\n",
            " Epoch 31 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.739\n",
            "Validation Loss: 1.731\n",
            "\n",
            " Epoch 32 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.738\n",
            "Validation Loss: 1.730\n",
            "\n",
            " Epoch 33 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.734\n",
            "Validation Loss: 1.728\n",
            "\n",
            " Epoch 34 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.735\n",
            "Validation Loss: 1.726\n",
            "\n",
            " Epoch 35 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.733\n",
            "Validation Loss: 1.725\n",
            "\n",
            " Epoch 36 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.735\n",
            "Validation Loss: 1.723\n",
            "\n",
            " Epoch 37 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.725\n",
            "Validation Loss: 1.722\n",
            "\n",
            " Epoch 38 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.728\n",
            "Validation Loss: 1.720\n",
            "\n",
            " Epoch 39 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.725\n",
            "Validation Loss: 1.719\n",
            "\n",
            " Epoch 40 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.722\n",
            "Validation Loss: 1.717\n",
            "\n",
            " Epoch 41 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.722\n",
            "Validation Loss: 1.716\n",
            "\n",
            " Epoch 42 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.720\n",
            "Validation Loss: 1.714\n",
            "\n",
            " Epoch 43 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.720\n",
            "Validation Loss: 1.713\n",
            "\n",
            " Epoch 44 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.721\n",
            "Validation Loss: 1.712\n",
            "\n",
            " Epoch 45 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.720\n",
            "Validation Loss: 1.710\n",
            "\n",
            " Epoch 46 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.718\n",
            "Validation Loss: 1.709\n",
            "\n",
            " Epoch 47 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.716\n",
            "Validation Loss: 1.707\n",
            "\n",
            " Epoch 48 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.717\n",
            "Validation Loss: 1.706\n",
            "\n",
            " Epoch 49 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.713\n",
            "Validation Loss: 1.705\n",
            "\n",
            " Epoch 50 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.710\n",
            "Validation Loss: 1.703\n",
            "\n",
            " Epoch 51 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.712\n",
            "Validation Loss: 1.702\n",
            "\n",
            " Epoch 52 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.710\n",
            "Validation Loss: 1.700\n",
            "\n",
            " Epoch 53 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.708\n",
            "Validation Loss: 1.699\n",
            "\n",
            " Epoch 54 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.705\n",
            "Validation Loss: 1.698\n",
            "\n",
            " Epoch 55 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.711\n",
            "Validation Loss: 1.696\n",
            "\n",
            " Epoch 56 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.703\n",
            "Validation Loss: 1.694\n",
            "\n",
            " Epoch 57 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.706\n",
            "Validation Loss: 1.693\n",
            "\n",
            " Epoch 58 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.703\n",
            "Validation Loss: 1.691\n",
            "\n",
            " Epoch 59 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.697\n",
            "Validation Loss: 1.690\n",
            "\n",
            " Epoch 60 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.701\n",
            "Validation Loss: 1.688\n",
            "\n",
            " Epoch 61 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.697\n",
            "Validation Loss: 1.687\n",
            "\n",
            " Epoch 62 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.698\n",
            "Validation Loss: 1.686\n",
            "\n",
            " Epoch 63 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.696\n",
            "Validation Loss: 1.684\n",
            "\n",
            " Epoch 64 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.694\n",
            "Validation Loss: 1.683\n",
            "\n",
            " Epoch 65 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.695\n",
            "Validation Loss: 1.682\n",
            "\n",
            " Epoch 66 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.695\n",
            "Validation Loss: 1.681\n",
            "\n",
            " Epoch 67 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.698\n",
            "Validation Loss: 1.680\n",
            "\n",
            " Epoch 68 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.695\n",
            "Validation Loss: 1.678\n",
            "\n",
            " Epoch 69 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.693\n",
            "Validation Loss: 1.677\n",
            "\n",
            " Epoch 70 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.691\n",
            "Validation Loss: 1.675\n",
            "\n",
            " Epoch 71 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.687\n",
            "Validation Loss: 1.674\n",
            "\n",
            " Epoch 72 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.686\n",
            "Validation Loss: 1.673\n",
            "\n",
            " Epoch 73 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.687\n",
            "Validation Loss: 1.672\n",
            "\n",
            " Epoch 74 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.687\n",
            "Validation Loss: 1.671\n",
            "\n",
            " Epoch 75 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.686\n",
            "Validation Loss: 1.670\n",
            "\n",
            " Epoch 76 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.686\n",
            "Validation Loss: 1.669\n",
            "\n",
            " Epoch 77 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.687\n",
            "Validation Loss: 1.668\n",
            "\n",
            " Epoch 78 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.679\n",
            "Validation Loss: 1.667\n",
            "\n",
            " Epoch 79 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.687\n",
            "Validation Loss: 1.666\n",
            "\n",
            " Epoch 80 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.680\n",
            "Validation Loss: 1.664\n",
            "\n",
            " Epoch 81 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.673\n",
            "Validation Loss: 1.663\n",
            "\n",
            " Epoch 82 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.676\n",
            "Validation Loss: 1.662\n",
            "\n",
            " Epoch 83 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.677\n",
            "Validation Loss: 1.661\n",
            "\n",
            " Epoch 84 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.673\n",
            "Validation Loss: 1.660\n",
            "\n",
            " Epoch 85 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.677\n",
            "Validation Loss: 1.659\n",
            "\n",
            " Epoch 86 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.675\n",
            "Validation Loss: 1.657\n",
            "\n",
            " Epoch 87 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.671\n",
            "Validation Loss: 1.656\n",
            "\n",
            " Epoch 88 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.674\n",
            "Validation Loss: 1.655\n",
            "\n",
            " Epoch 89 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.673\n",
            "Validation Loss: 1.654\n",
            "\n",
            " Epoch 90 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.668\n",
            "Validation Loss: 1.653\n",
            "\n",
            " Epoch 91 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.668\n",
            "Validation Loss: 1.652\n",
            "\n",
            " Epoch 92 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.671\n",
            "Validation Loss: 1.651\n",
            "\n",
            " Epoch 93 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.669\n",
            "Validation Loss: 1.651\n",
            "\n",
            " Epoch 94 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.669\n",
            "Validation Loss: 1.650\n",
            "\n",
            " Epoch 95 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.666\n",
            "Validation Loss: 1.648\n",
            "\n",
            " Epoch 96 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.665\n",
            "Validation Loss: 1.647\n",
            "\n",
            " Epoch 97 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.660\n",
            "Validation Loss: 1.646\n",
            "\n",
            " Epoch 98 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.663\n",
            "Validation Loss: 1.644\n",
            "\n",
            " Epoch 99 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.654\n",
            "Validation Loss: 1.643\n",
            "\n",
            " Epoch 100 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.661\n",
            "Validation Loss: 1.642\n",
            "\n",
            " Epoch 101 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.658\n",
            "Validation Loss: 1.641\n",
            "\n",
            " Epoch 102 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.657\n",
            "Validation Loss: 1.640\n",
            "\n",
            " Epoch 103 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.656\n",
            "Validation Loss: 1.639\n",
            "\n",
            " Epoch 104 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.661\n",
            "Validation Loss: 1.638\n",
            "\n",
            " Epoch 105 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.655\n",
            "Validation Loss: 1.638\n",
            "\n",
            " Epoch 106 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.657\n",
            "Validation Loss: 1.637\n",
            "\n",
            " Epoch 107 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.653\n",
            "Validation Loss: 1.636\n",
            "\n",
            " Epoch 108 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.654\n",
            "Validation Loss: 1.634\n",
            "\n",
            " Epoch 109 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.651\n",
            "Validation Loss: 1.633\n",
            "\n",
            " Epoch 110 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.657\n",
            "Validation Loss: 1.631\n",
            "\n",
            " Epoch 111 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.657\n",
            "Validation Loss: 1.630\n",
            "\n",
            " Epoch 112 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.648\n",
            "Validation Loss: 1.629\n",
            "\n",
            " Epoch 113 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.648\n",
            "Validation Loss: 1.628\n",
            "\n",
            " Epoch 114 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.650\n",
            "Validation Loss: 1.628\n",
            "\n",
            " Epoch 115 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.646\n",
            "Validation Loss: 1.627\n",
            "\n",
            " Epoch 116 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.647\n",
            "Validation Loss: 1.626\n",
            "\n",
            " Epoch 117 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.653\n",
            "Validation Loss: 1.625\n",
            "\n",
            " Epoch 118 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.648\n",
            "Validation Loss: 1.624\n",
            "\n",
            " Epoch 119 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.652\n",
            "Validation Loss: 1.623\n",
            "\n",
            " Epoch 120 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.648\n",
            "Validation Loss: 1.622\n",
            "\n",
            " Epoch 121 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.644\n",
            "Validation Loss: 1.622\n",
            "\n",
            " Epoch 122 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.646\n",
            "Validation Loss: 1.621\n",
            "\n",
            " Epoch 123 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.640\n",
            "Validation Loss: 1.620\n",
            "\n",
            " Epoch 124 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.643\n",
            "Validation Loss: 1.619\n",
            "\n",
            " Epoch 125 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.644\n",
            "Validation Loss: 1.618\n",
            "\n",
            " Epoch 126 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.641\n",
            "Validation Loss: 1.617\n",
            "\n",
            " Epoch 127 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.642\n",
            "Validation Loss: 1.616\n",
            "\n",
            " Epoch 128 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.647\n",
            "Validation Loss: 1.615\n",
            "\n",
            " Epoch 129 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.640\n",
            "Validation Loss: 1.615\n",
            "\n",
            " Epoch 130 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.642\n",
            "Validation Loss: 1.614\n",
            "\n",
            " Epoch 131 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.637\n",
            "Validation Loss: 1.613\n",
            "\n",
            " Epoch 132 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.640\n",
            "Validation Loss: 1.612\n",
            "\n",
            " Epoch 133 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.634\n",
            "Validation Loss: 1.611\n",
            "\n",
            " Epoch 134 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.628\n",
            "Validation Loss: 1.611\n",
            "\n",
            " Epoch 135 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.625\n",
            "Validation Loss: 1.610\n",
            "\n",
            " Epoch 136 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.639\n",
            "Validation Loss: 1.609\n",
            "\n",
            " Epoch 137 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.621\n",
            "Validation Loss: 1.608\n",
            "\n",
            " Epoch 138 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.628\n",
            "Validation Loss: 1.607\n",
            "\n",
            " Epoch 139 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.631\n",
            "Validation Loss: 1.605\n",
            "\n",
            " Epoch 140 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.629\n",
            "Validation Loss: 1.604\n",
            "\n",
            " Epoch 141 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.631\n",
            "Validation Loss: 1.603\n",
            "\n",
            " Epoch 142 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.638\n",
            "Validation Loss: 1.603\n",
            "\n",
            " Epoch 143 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.629\n",
            "Validation Loss: 1.602\n",
            "\n",
            " Epoch 144 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.629\n",
            "Validation Loss: 1.602\n",
            "\n",
            " Epoch 145 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.622\n",
            "Validation Loss: 1.601\n",
            "\n",
            " Epoch 146 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.631\n",
            "Validation Loss: 1.600\n",
            "\n",
            " Epoch 147 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.627\n",
            "Validation Loss: 1.599\n",
            "\n",
            " Epoch 148 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.626\n",
            "Validation Loss: 1.598\n",
            "\n",
            " Epoch 149 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.635\n",
            "Validation Loss: 1.597\n",
            "\n",
            " Epoch 150 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.626\n",
            "Validation Loss: 1.597\n",
            "\n",
            " Epoch 151 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.624\n",
            "Validation Loss: 1.597\n",
            "\n",
            " Epoch 152 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.630\n",
            "Validation Loss: 1.596\n",
            "\n",
            " Epoch 153 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.623\n",
            "Validation Loss: 1.595\n",
            "\n",
            " Epoch 154 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.630\n",
            "Validation Loss: 1.594\n",
            "\n",
            " Epoch 155 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.628\n",
            "Validation Loss: 1.594\n",
            "\n",
            " Epoch 156 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.619\n",
            "Validation Loss: 1.593\n",
            "\n",
            " Epoch 157 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.622\n",
            "Validation Loss: 1.592\n",
            "\n",
            " Epoch 158 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.623\n",
            "Validation Loss: 1.592\n",
            "\n",
            " Epoch 159 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.619\n",
            "Validation Loss: 1.591\n",
            "\n",
            " Epoch 160 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.622\n",
            "Validation Loss: 1.591\n",
            "\n",
            " Epoch 161 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.621\n",
            "Validation Loss: 1.591\n",
            "\n",
            " Epoch 162 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.615\n",
            "Validation Loss: 1.589\n",
            "\n",
            " Epoch 163 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.621\n",
            "Validation Loss: 1.588\n",
            "\n",
            " Epoch 164 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.619\n",
            "Validation Loss: 1.587\n",
            "\n",
            " Epoch 165 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.612\n",
            "Validation Loss: 1.586\n",
            "\n",
            " Epoch 166 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.618\n",
            "Validation Loss: 1.585\n",
            "\n",
            " Epoch 167 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.621\n",
            "Validation Loss: 1.585\n",
            "\n",
            " Epoch 168 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.608\n",
            "Validation Loss: 1.584\n",
            "\n",
            " Epoch 169 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.614\n",
            "Validation Loss: 1.583\n",
            "\n",
            " Epoch 170 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.616\n",
            "Validation Loss: 1.583\n",
            "\n",
            " Epoch 171 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.616\n",
            "Validation Loss: 1.582\n",
            "\n",
            " Epoch 172 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.611\n",
            "Validation Loss: 1.582\n",
            "\n",
            " Epoch 173 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.616\n",
            "Validation Loss: 1.581\n",
            "\n",
            " Epoch 174 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.614\n",
            "Validation Loss: 1.581\n",
            "\n",
            " Epoch 175 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.612\n",
            "Validation Loss: 1.580\n",
            "\n",
            " Epoch 176 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.609\n",
            "Validation Loss: 1.580\n",
            "\n",
            " Epoch 177 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.604\n",
            "Validation Loss: 1.580\n",
            "\n",
            " Epoch 178 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.617\n",
            "Validation Loss: 1.580\n",
            "\n",
            " Epoch 179 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.607\n",
            "Validation Loss: 1.579\n",
            "\n",
            " Epoch 180 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.605\n",
            "Validation Loss: 1.578\n",
            "\n",
            " Epoch 181 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.613\n",
            "Validation Loss: 1.576\n",
            "\n",
            " Epoch 182 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.613\n",
            "Validation Loss: 1.575\n",
            "\n",
            " Epoch 183 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.607\n",
            "Validation Loss: 1.575\n",
            "\n",
            " Epoch 184 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.606\n",
            "Validation Loss: 1.574\n",
            "\n",
            " Epoch 185 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.607\n",
            "Validation Loss: 1.574\n",
            "\n",
            " Epoch 186 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.607\n",
            "Validation Loss: 1.574\n",
            "\n",
            " Epoch 187 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.608\n",
            "Validation Loss: 1.573\n",
            "\n",
            " Epoch 188 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.609\n",
            "Validation Loss: 1.573\n",
            "\n",
            " Epoch 189 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.598\n",
            "Validation Loss: 1.572\n",
            "\n",
            " Epoch 190 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.602\n",
            "Validation Loss: 1.571\n",
            "\n",
            " Epoch 191 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.597\n",
            "Validation Loss: 1.570\n",
            "\n",
            " Epoch 192 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.597\n",
            "Validation Loss: 1.570\n",
            "\n",
            " Epoch 193 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.598\n",
            "Validation Loss: 1.569\n",
            "\n",
            " Epoch 194 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.596\n",
            "Validation Loss: 1.568\n",
            "\n",
            " Epoch 195 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.606\n",
            "Validation Loss: 1.568\n",
            "\n",
            " Epoch 196 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.613\n",
            "Validation Loss: 1.567\n",
            "\n",
            " Epoch 197 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.606\n",
            "Validation Loss: 1.567\n",
            "\n",
            " Epoch 198 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.606\n",
            "Validation Loss: 1.566\n",
            "\n",
            " Epoch 199 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.601\n",
            "Validation Loss: 1.566\n",
            "\n",
            " Epoch 200 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.602\n",
            "Validation Loss: 1.565\n",
            "\n",
            " Epoch 201 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.597\n",
            "Validation Loss: 1.565\n",
            "\n",
            " Epoch 202 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.597\n",
            "Validation Loss: 1.565\n",
            "\n",
            " Epoch 203 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.606\n",
            "Validation Loss: 1.564\n",
            "\n",
            " Epoch 204 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.597\n",
            "Validation Loss: 1.564\n",
            "\n",
            " Epoch 205 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.601\n",
            "Validation Loss: 1.563\n",
            "\n",
            " Epoch 206 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.598\n",
            "Validation Loss: 1.562\n",
            "\n",
            " Epoch 207 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.595\n",
            "Validation Loss: 1.562\n",
            "\n",
            " Epoch 208 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.587\n",
            "Validation Loss: 1.561\n",
            "\n",
            " Epoch 209 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.597\n",
            "Validation Loss: 1.560\n",
            "\n",
            " Epoch 210 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.595\n",
            "Validation Loss: 1.559\n",
            "\n",
            " Epoch 211 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.592\n",
            "Validation Loss: 1.558\n",
            "\n",
            " Epoch 212 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.597\n",
            "Validation Loss: 1.558\n",
            "\n",
            " Epoch 213 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.601\n",
            "Validation Loss: 1.558\n",
            "\n",
            " Epoch 214 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.599\n",
            "Validation Loss: 1.558\n",
            "\n",
            " Epoch 215 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.594\n",
            "Validation Loss: 1.558\n",
            "\n",
            " Epoch 216 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.591\n",
            "Validation Loss: 1.557\n",
            "\n",
            " Epoch 217 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.585\n",
            "Validation Loss: 1.557\n",
            "\n",
            " Epoch 218 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.592\n",
            "Validation Loss: 1.556\n",
            "\n",
            " Epoch 219 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.593\n",
            "Validation Loss: 1.555\n",
            "\n",
            " Epoch 220 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.593\n",
            "Validation Loss: 1.555\n",
            "\n",
            " Epoch 221 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.590\n",
            "Validation Loss: 1.554\n",
            "\n",
            " Epoch 222 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.594\n",
            "Validation Loss: 1.553\n",
            "\n",
            " Epoch 223 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.591\n",
            "Validation Loss: 1.553\n",
            "\n",
            " Epoch 224 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.589\n",
            "Validation Loss: 1.552\n",
            "\n",
            " Epoch 225 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.595\n",
            "Validation Loss: 1.552\n",
            "\n",
            " Epoch 226 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.590\n",
            "Validation Loss: 1.552\n",
            "\n",
            " Epoch 227 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.586\n",
            "Validation Loss: 1.551\n",
            "\n",
            " Epoch 228 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.589\n",
            "Validation Loss: 1.551\n",
            "\n",
            " Epoch 229 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.589\n",
            "Validation Loss: 1.550\n",
            "\n",
            " Epoch 230 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.588\n",
            "Validation Loss: 1.550\n",
            "\n",
            " Epoch 231 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.586\n",
            "Validation Loss: 1.549\n",
            "\n",
            " Epoch 232 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.589\n",
            "Validation Loss: 1.549\n",
            "\n",
            " Epoch 233 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.588\n",
            "Validation Loss: 1.549\n",
            "\n",
            " Epoch 234 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.578\n",
            "Validation Loss: 1.548\n",
            "\n",
            " Epoch 235 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.579\n",
            "Validation Loss: 1.547\n",
            "\n",
            " Epoch 236 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.581\n",
            "Validation Loss: 1.546\n",
            "\n",
            " Epoch 237 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.584\n",
            "Validation Loss: 1.546\n",
            "\n",
            " Epoch 238 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.579\n",
            "Validation Loss: 1.546\n",
            "\n",
            " Epoch 239 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.578\n",
            "Validation Loss: 1.546\n",
            "\n",
            " Epoch 240 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.581\n",
            "Validation Loss: 1.545\n",
            "\n",
            " Epoch 241 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.581\n",
            "Validation Loss: 1.544\n",
            "\n",
            " Epoch 242 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.582\n",
            "Validation Loss: 1.543\n",
            "\n",
            " Epoch 243 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.578\n",
            "Validation Loss: 1.543\n",
            "\n",
            " Epoch 244 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.581\n",
            "Validation Loss: 1.542\n",
            "\n",
            " Epoch 245 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.583\n",
            "Validation Loss: 1.542\n",
            "\n",
            " Epoch 246 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.581\n",
            "Validation Loss: 1.541\n",
            "\n",
            " Epoch 247 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.575\n",
            "Validation Loss: 1.541\n",
            "\n",
            " Epoch 248 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.577\n",
            "Validation Loss: 1.540\n",
            "\n",
            " Epoch 249 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.577\n",
            "Validation Loss: 1.540\n",
            "\n",
            " Epoch 250 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.577\n",
            "Validation Loss: 1.539\n",
            "\n",
            " Epoch 251 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.569\n",
            "Validation Loss: 1.539\n",
            "\n",
            " Epoch 252 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.580\n",
            "Validation Loss: 1.538\n",
            "\n",
            " Epoch 253 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.582\n",
            "Validation Loss: 1.538\n",
            "\n",
            " Epoch 254 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.576\n",
            "Validation Loss: 1.538\n",
            "\n",
            " Epoch 255 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.583\n",
            "Validation Loss: 1.538\n",
            "\n",
            " Epoch 256 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.568\n",
            "Validation Loss: 1.537\n",
            "\n",
            " Epoch 257 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.578\n",
            "Validation Loss: 1.536\n",
            "\n",
            " Epoch 258 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.572\n",
            "Validation Loss: 1.536\n",
            "\n",
            " Epoch 259 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.575\n",
            "Validation Loss: 1.535\n",
            "\n",
            " Epoch 260 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.571\n",
            "Validation Loss: 1.534\n",
            "\n",
            " Epoch 261 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.574\n",
            "Validation Loss: 1.534\n",
            "\n",
            " Epoch 262 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.579\n",
            "Validation Loss: 1.534\n",
            "\n",
            " Epoch 263 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.560\n",
            "Validation Loss: 1.534\n",
            "\n",
            " Epoch 264 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.571\n",
            "Validation Loss: 1.533\n",
            "\n",
            " Epoch 265 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.571\n",
            "Validation Loss: 1.533\n",
            "\n",
            " Epoch 266 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.575\n",
            "Validation Loss: 1.532\n",
            "\n",
            " Epoch 267 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.575\n",
            "Validation Loss: 1.532\n",
            "\n",
            " Epoch 268 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.573\n",
            "Validation Loss: 1.532\n",
            "\n",
            " Epoch 269 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.574\n",
            "Validation Loss: 1.532\n",
            "\n",
            " Epoch 270 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.574\n",
            "Validation Loss: 1.532\n",
            "\n",
            " Epoch 271 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.574\n",
            "Validation Loss: 1.532\n",
            "\n",
            " Epoch 272 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.577\n",
            "Validation Loss: 1.531\n",
            "\n",
            " Epoch 273 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.565\n",
            "Validation Loss: 1.531\n",
            "\n",
            " Epoch 274 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.567\n",
            "Validation Loss: 1.530\n",
            "\n",
            " Epoch 275 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.561\n",
            "Validation Loss: 1.530\n",
            "\n",
            " Epoch 276 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.567\n",
            "Validation Loss: 1.529\n",
            "\n",
            " Epoch 277 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.568\n",
            "Validation Loss: 1.528\n",
            "\n",
            " Epoch 278 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.565\n",
            "Validation Loss: 1.527\n",
            "\n",
            " Epoch 279 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.570\n",
            "Validation Loss: 1.527\n",
            "\n",
            " Epoch 280 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.562\n",
            "Validation Loss: 1.527\n",
            "\n",
            " Epoch 281 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.569\n",
            "Validation Loss: 1.527\n",
            "\n",
            " Epoch 282 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.565\n",
            "Validation Loss: 1.527\n",
            "\n",
            " Epoch 283 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.571\n",
            "Validation Loss: 1.526\n",
            "\n",
            " Epoch 284 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.566\n",
            "Validation Loss: 1.525\n",
            "\n",
            " Epoch 285 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.564\n",
            "Validation Loss: 1.525\n",
            "\n",
            " Epoch 286 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.571\n",
            "Validation Loss: 1.525\n",
            "\n",
            " Epoch 287 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.566\n",
            "Validation Loss: 1.525\n",
            "\n",
            " Epoch 288 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.575\n",
            "Validation Loss: 1.524\n",
            "\n",
            " Epoch 289 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.564\n",
            "Validation Loss: 1.523\n",
            "\n",
            " Epoch 290 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.571\n",
            "Validation Loss: 1.523\n",
            "\n",
            " Epoch 291 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.556\n",
            "Validation Loss: 1.523\n",
            "\n",
            " Epoch 292 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.556\n",
            "Validation Loss: 1.523\n",
            "\n",
            " Epoch 293 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.557\n",
            "Validation Loss: 1.523\n",
            "\n",
            " Epoch 294 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.558\n",
            "Validation Loss: 1.522\n",
            "\n",
            " Epoch 295 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.570\n",
            "Validation Loss: 1.522\n",
            "\n",
            " Epoch 296 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.559\n",
            "Validation Loss: 1.521\n",
            "\n",
            " Epoch 297 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.557\n",
            "Validation Loss: 1.521\n",
            "\n",
            " Epoch 298 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.561\n",
            "Validation Loss: 1.520\n",
            "\n",
            " Epoch 299 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.565\n",
            "Validation Loss: 1.520\n",
            "\n",
            " Epoch 300 / 300\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.571\n",
            "Validation Loss: 1.520\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BxWEFVXDtmG",
        "outputId": "621ce467-3a27-4f3f-89df-21d56451f1a9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## HERE WE MAKING PREDICTIONS\n",
        "\n",
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\n",
        "    preds = preds.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "-MjUxPoSDxfi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model's performance\n",
        "preds = np.argmax(preds, axis = 1)\n",
        "\n",
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiDm1VFhD1Se",
        "outputId": "4f2e386a-71d4-4aa6-8385-e66404955925"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      1.00      0.84         8\n",
            "           1       0.50      0.38      0.43         8\n",
            "           2       0.25      0.12      0.17         8\n",
            "           3       0.29      0.25      0.27         8\n",
            "           4       0.31      0.62      0.42         8\n",
            "           5       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.40        48\n",
            "   macro avg       0.35      0.40      0.35        48\n",
            "weighted avg       0.35      0.40      0.35        48\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive Bayes \n",
        "0.4822916666666667\n",
        "\n",
        "#SVM\n",
        "#0.4583333333333333\n",
        "\n",
        "\n",
        "#BERT\n",
        "#Batch 32\n",
        "#epoch 10\n",
        "#Accuracy 0.26\n",
        "\n",
        "#Batch 1000\n",
        "#epoch 1000\n",
        "#Accuracy:\n",
        "# 0.32\n",
        "\n",
        "#Batch 1000\n",
        "#epoch100\n",
        "#tokenization 50\n",
        "#0.33 \n",
        "\n",
        "#Batch 10000\n",
        "#epoch 50\n",
        "#tokenization 100\n",
        "#0.35 \n",
        "\n",
        "#Batch 1000\n",
        "#epoch 300\n",
        "#tokenization 150\n",
        "#0.40 \n",
        "\n"
      ],
      "metadata": {
        "id": "3gs9cNeNGj6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test also so this \n",
        "#LR_cv = LogisticRegressionCV(solver='sag', cv=10, max_iter=100, random_state = 0, warm_start= bool,)"
      ],
      "metadata": {
        "id": "sktG_K5RloBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive bayes test\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n"
      ],
      "metadata": {
        "id": "GIa_hocGPcpo",
        "outputId": "e0e64666-4b8c-4463-e7a8-c5081d4c3ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-c0808a338b59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m               ])\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# Fit or load from cache the current transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    349\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \"\"\"\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: lower not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST Support Vector Machine\n",
        "#Split \n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "#Vectorize\n",
        "texts = df['sentence']\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names())\n",
        "\n",
        "#Transform\n",
        "text_transformer = TfidfVectorizer(ngram_range=(1, 2), lowercase=True, max_features=150000)\n",
        "X_train_text = text_transformer.fit_transform(X_train)\n",
        "X_test_text = text_transformer.transform(X_test)\n",
        "\n",
        "#Split here? \n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "               ])\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = sgd.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "\n",
        "#0.4583333333333333"
      ],
      "metadata": {
        "id": "kCyGMN6cO4qK",
        "outputId": "c879e629-19f9-4f39-d53d-33da2771ca7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.4583333333333333\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Project_guidelines_2022.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}